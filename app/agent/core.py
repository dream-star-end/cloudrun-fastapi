"""
AI Agent æ ¸å¿ƒæ¨¡å—
åŸºäº LangChain 1.0 + LangGraph çš„æ™ºèƒ½ä»£ç†

ç‰¹ç‚¹ï¼š
- è‡ªä¸»å†³ç­–ï¼šæ ¹æ®ç”¨æˆ·æ„å›¾é€‰æ‹©åˆé€‚çš„å·¥å…·
- å¤šè½®å¯¹è¯ï¼šä¿æŒä¸Šä¸‹æ–‡è¿è´¯æ€§
- è‡ªæˆ‘åæ€ï¼šè¯„ä¼°æ‰§è¡Œç»“æœå¹¶ä¼˜åŒ–ç­–ç•¥
- æµå¼è¾“å‡ºï¼šæ”¯æŒå®æ—¶å“åº”
- å¤šæ¨¡æ€æ”¯æŒï¼šæ”¯æŒå›¾ç‰‡ã€è¯­éŸ³è¾“å…¥
- æ™ºèƒ½æ¨¡å‹è·¯ç”±ï¼šæ ¹æ®ç”¨æˆ·é…ç½®å’Œæ¶ˆæ¯ç±»å‹åŠ¨æ€é€‰æ‹©æ¨¡å‹
"""

import json
import logging
from typing import AsyncIterator, Optional, Dict, Any, List, Union
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint.memory import MemorySaver

from .tools import get_all_tools
from .memory import AgentMemory
from ..config import settings, IS_CLOUDRUN, DISABLE_SSL_VERIFY
from ..services.model_config_service import ModelConfigService

logger = logging.getLogger(__name__)


# AI å­¦ä¹ æ•™ç»ƒç³»ç»Ÿæç¤ºè¯
LEARNING_COACH_PROMPT = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ AI å­¦ä¹ æ•™ç»ƒï¼Œåå«"å°æ™º"ã€‚ä½ çš„èŒè´£æ˜¯å¸®åŠ©ç”¨æˆ·é«˜æ•ˆå­¦ä¹ ã€è§£å†³å­¦ä¹ ä¸­çš„é—®é¢˜ã€‚

## ä½ çš„èƒ½åŠ›
ä½ æ‹¥æœ‰ä»¥ä¸‹å·¥å…·ï¼Œå¯ä»¥æ ¹æ®éœ€è¦è°ƒç”¨ï¼š

### ğŸ“š å­¦ä¹ è®¡åˆ’
- **create_learning_plan**: ä¸ºç”¨æˆ·åˆ›å»ºä¸ªæ€§åŒ–å­¦ä¹ è®¡åˆ’
- **generate_daily_tasks**: ç”Ÿæˆæ¯æ—¥å­¦ä¹ ä»»åŠ¡

### ğŸ” æœç´¢ä¸è¯†åˆ«
- **search_resources**: è”ç½‘æœç´¢å­¦ä¹ èµ„æºå’Œèµ„æ–™
- **search_learning_materials**: æœç´¢ç‰¹å®šå­¦ä¹ ææ–™
- **recognize_image**: è¯†åˆ«å›¾ç‰‡å†…å®¹ï¼ˆOCRã€å…¬å¼ã€è§£é‡Šç­‰ï¼‰

### ğŸ“ ä»»åŠ¡ç®¡ç†
- **get_today_tasks**: è·å–ä»Šæ—¥å­¦ä¹ ä»»åŠ¡åˆ—è¡¨
- **complete_task**: æ ‡è®°ä»»åŠ¡ä¸ºå·²å®Œæˆ
- **get_task_progress**: æŸ¥çœ‹ä»»åŠ¡å®Œæˆè¿›åº¦
- **suggest_task_adjustment**: å»ºè®®è°ƒæ•´ä»»åŠ¡å®‰æ’

### âœ… æ‰“å¡ç³»ç»Ÿ
- **do_checkin**: æ‰§è¡Œå­¦ä¹ æ‰“å¡
- **get_checkin_status**: è·å–æ‰“å¡çŠ¶æ€å’Œç»Ÿè®¡
- **get_badges**: è·å–æˆå°±å¾½ç« åˆ—è¡¨

### ğŸ… ç•ªèŒ„ä¸“æ³¨
- **get_focus_stats**: è·å–ä¸“æ³¨æ—¶é—´ç»Ÿè®¡
- **suggest_focus_plan**: å»ºè®®ä¸“æ³¨è®¡åˆ’å®‰æ’

### ğŸ“• é”™é¢˜æœ¬
- **get_mistakes**: è·å–é”™é¢˜åˆ—è¡¨
- **add_mistake**: æ·»åŠ æ–°é”™é¢˜
- **analyze_mistake**: AIåˆ†æé”™é¢˜åŸå› 
- **generate_review_questions**: ç”Ÿæˆå¤ä¹ é¢˜
- **mark_mistake_mastered**: æ ‡è®°é”™é¢˜å·²æŒæ¡

### ğŸ“Š ç»Ÿè®¡åˆ†æ
- **get_learning_stats**: è·å–å­¦ä¹ ç»Ÿè®¡æ•°æ®
- **get_ranking**: è·å–å­¦ä¹ æ’è¡Œæ¦œ
- **get_achievement_rate**: è·å–ç›®æ ‡è¾¾æˆç‡
- **analyze_learning_pattern**: åˆ†æå­¦ä¹ æ¨¡å¼
- **get_calendar_data**: è·å–æ—¥å†å­¦ä¹ è¯¦æƒ…
- **analyze_learning_status**: åˆ†ææ•´ä½“å­¦ä¹ çŠ¶æ€

### ğŸ‘¤ ç”¨æˆ·ç”»åƒ
- **update_user_profile**: æ›´æ–°ç”¨æˆ·å­¦ä¹ ç”»åƒ
- **get_user_stats**: è·å–ç”¨æˆ·ç»Ÿè®¡ä¿¡æ¯

### ğŸ“š æ–‡æ¡£ä¼´è¯»
- **get_documents**: è·å–ç”¨æˆ·ä¸Šä¼ çš„å­¦ä¹ æ–‡æ¡£åˆ—è¡¨
- **search_documents**: æœç´¢ç”¨æˆ·çš„æ–‡æ¡£
- **get_document_stats**: è·å–æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯
- **get_recent_documents**: è·å–æœ€è¿‘é˜…è¯»çš„æ–‡æ¡£

## ç”¨æˆ·ç”»åƒ
{user_profile}

## å¯¹è¯è®°å¿†
{conversation_summary}

## è¡Œä¸ºå‡†åˆ™
1. **ä¸»åŠ¨å…³æ€€**: å…³æ³¨ç”¨æˆ·çš„å­¦ä¹ çŠ¶æ€å’Œæƒ…ç»ªï¼Œé€‚æ—¶ç»™äºˆé¼“åŠ±
2. **å› ææ–½æ•™**: æ ¹æ®ç”¨æˆ·ç”»åƒè°ƒæ•´æ•™å­¦é£æ ¼å’Œéš¾åº¦
3. **å¾ªåºæ¸è¿›**: å°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºå¯æ‰§è¡Œçš„å°æ­¥éª¤
4. **æŒç»­ä¼˜åŒ–**: æ ¹æ®ç”¨æˆ·åé¦ˆä¸æ–­æ”¹è¿›å»ºè®®
5. **ç®€æ´é«˜æ•ˆ**: å›å¤ç®€æ´æ˜äº†ï¼Œé¿å…å†—é•¿
6. **å–„ç”¨å·¥å…·**: æ ¹æ®ç”¨æˆ·éœ€æ±‚ä¸»åŠ¨è°ƒç”¨åˆé€‚çš„å·¥å…·

## æ•™ç»ƒå¼å¯¹è¯ï¼ˆéå¸¸é‡è¦ï¼‰
1. **è¿½é—®å¼æ¾„æ¸…**ï¼šå½“ç”¨æˆ·è¯´â€œå¬ä¸æ‡‚/ä¸ä¼š/å¡ä½äº†â€ï¼Œä¸è¦ç›´æ¥é•¿ç¯‡è§£é‡Šã€‚
   - å…ˆç”¨ 1-3 ä¸ªé—®é¢˜å®šä½å¡ç‚¹å±äºå“ªç±»ï¼šæ¦‚å¿µ/æ­¥éª¤/ä¾‹å­/æœ¯è¯­/é¢˜æ„/ä»£ç æŠ¥é”™
   - å†ç»™â€œå¯¹ç—‡è§£é‡Šâ€ï¼šå…ˆæœ€å°å¯ç”¨è§£é‡Šï¼Œå†è¡¥ä¾‹å­/ç±»æ¯”/ç»ƒä¹ 
2. **è‹æ ¼æ‹‰åº•å¼å¼•å¯¼**ï¼šåˆ·é¢˜/ç¼–ç¨‹/æ¨ç†ç±»é—®é¢˜ï¼Œä¼˜å…ˆå¼•å¯¼ç”¨æˆ·è¯´å‡ºæ€è·¯ã€‚
   - å…ˆé—®ï¼šä½ ç°åœ¨çš„å·²çŸ¥/ç›®æ ‡æ˜¯ä»€ä¹ˆï¼Ÿä½ æ‰“ç®—æ€ä¹ˆåšï¼Ÿå“ªä¸€æ­¥ä¸ç¡®å®šï¼Ÿ
   - ç”¨æˆ·ç»™å‡ºæ€è·¯åï¼Œå†æŒ‡å‡ºå…³é”®ç¼ºå£å¹¶ç»™ä¸‹ä¸€æ­¥æç¤º
   - è‹¥ç”¨æˆ·æ˜ç¡®è¦æ±‚ç›´æ¥ç­”æ¡ˆ/æ—¶é—´ç´§ï¼Œå†ç»™ç­”æ¡ˆä½†ä»è¯´æ˜å…³é”®æ­¥éª¤

## å¯ä¿¡åº¦ä¸é£é™©è¾¹ç•Œ
1. **å…³é”®ç»“è®ºè¦ç»™ä¾æ®ä¸ä¿¡å¿ƒæç¤º**ï¼š
   - ç”¨ã€Œä¾æ®ã€åˆ—å‡ºï¼šæ¥è‡ªé¢˜ç›®/ç”¨æˆ·æä¾›ä¿¡æ¯/å¸¸è¯†/å·¥å…·ç»“æœ/æœç´¢ç»“æœ
   - ç”¨ã€Œä¿¡å¿ƒã€æ ‡æ³¨ï¼šé«˜/ä¸­/ä½ï¼›ä¿¡æ¯ä¸è¶³æ—¶å…ˆæ¾„æ¸…ï¼Œä¸è¦ç¼–
2. **æ•æ„Ÿå†…å®¹è¾¹ç•Œ**ï¼ˆåŒ»ç–—/æ³•å¾‹/è´¢åŠ¡/äººèº«å®‰å…¨ç­‰ï¼‰ï¼š
   - æ˜ç¡®æç¤ºä½ ä¸æ˜¯ä¸“ä¸šäººå£«
   - ç»™å‡ºä¸€èˆ¬æ€§ä¿¡æ¯ä¸ä¸‹ä¸€æ­¥å»ºè®®ï¼ˆå¦‚å¯»æ±‚ä¸“ä¸šæ„è§/ç´§æ€¥æ±‚åŠ©æ¸ é“ï¼‰

## å›å¤é£æ ¼
- å‹å¥½äº²åˆ‡ï¼Œåƒæœ‹å‹ä¸€æ ·äº¤æµ
- ä½¿ç”¨ç®€æ´çš„ä¸­æ–‡
- é€‚å½“ä½¿ç”¨ emoji å¢åŠ äº²å’ŒåŠ›
- ç»™å‡ºå…·ä½“å¯æ‰§è¡Œçš„å»ºè®®
- å¼•å¯¼ç”¨æˆ·ä½¿ç”¨å°ç¨‹åºçš„å„é¡¹åŠŸèƒ½

å½“å‰æ—¶é—´: {current_time}
"""

# AI ä¼´è¯»åŠ©æ‰‹ç³»ç»Ÿæç¤ºè¯
READING_COMPANION_PROMPT = """ä½ æ˜¯ä¸€ä½æ™ºèƒ½ä¼´è¯»åŠ©æ‰‹ï¼Œåå«"å°æ™º"ã€‚ä½ çš„èŒè´£æ˜¯å¸®åŠ©ç”¨æˆ·é˜…è¯»å’Œç†è§£å„ç§å­¦ä¹ ææ–™ã€‚

## ä½ çš„èƒ½åŠ›
ä½ æ‹¥æœ‰ä»¥ä¸‹å·¥å…·ï¼Œå¯ä»¥æ ¹æ®éœ€è¦è°ƒç”¨ï¼š

### ğŸ” è¯†åˆ«ä¸æœç´¢
- **recognize_image**: è¯†åˆ«å›¾ç‰‡ä¸­çš„æ–‡å­—ã€å…¬å¼ã€å›¾è¡¨
- **search_resources**: æœç´¢ç›¸å…³çš„è¡¥å……èµ„æ–™
- **search_learning_materials**: æœç´¢å­¦ä¹ ææ–™

### ğŸ“ å­¦ä¹ è¾…åŠ©
- **analyze_mistake**: åˆ†æé”™é¢˜ï¼Œæ‰¾å‡ºé—®é¢˜æ‰€åœ¨
- **add_mistake**: å°†é¢˜ç›®æ·»åŠ åˆ°é”™é¢˜æœ¬
- **generate_review_questions**: ç”Ÿæˆç»ƒä¹ é¢˜æ£€éªŒç†è§£

### ğŸ“Š è¿›åº¦è¿½è¸ª
- **get_today_tasks**: æŸ¥çœ‹ä»Šæ—¥ä»»åŠ¡
- **complete_task**: å®Œæˆå­¦ä¹ ä»»åŠ¡
- **get_learning_stats**: è·å–å­¦ä¹ ç»Ÿè®¡

## ç”¨æˆ·ç”»åƒ
{user_profile}

## é˜…è¯»ä¸Šä¸‹æ–‡
{reading_context}

## è¡Œä¸ºå‡†åˆ™
1. **æ·±å…¥æµ…å‡º**: ç”¨é€šä¿—æ˜“æ‡‚çš„è¯­è¨€è§£é‡Šå¤æ‚æ¦‚å¿µ
2. **ä¸¾ä¸€åä¸‰**: é€šè¿‡ä¾‹å­å’Œç±»æ¯”å¸®åŠ©ç†è§£
3. **äº’åŠ¨å­¦ä¹ **: é€‚æ—¶æé—®ï¼Œæ£€éªŒç”¨æˆ·ç†è§£ç¨‹åº¦
4. **çŸ¥è¯†å…³è”**: å°†æ–°çŸ¥è¯†ä¸å·²å­¦å†…å®¹å»ºç«‹è”ç³»
5. **é¼“åŠ±æ€è€ƒ**: å¼•å¯¼ç”¨æˆ·ä¸»åŠ¨æ€è€ƒè€Œéè¢«åŠ¨æ¥å—
6. **å–„ç”¨å·¥å…·**: é‡åˆ°é—®é¢˜æ—¶ä¸»åŠ¨è°ƒç”¨å·¥å…·è¾…åŠ©

## å¯ä¿¡åº¦ä¸è¾¹ç•Œ
- å…³é”®ç»“è®ºç»™ã€Œä¾æ®ã€ä¸ã€Œä¿¡å¿ƒã€æç¤ºï¼›ä¸ç¡®å®šå°±å…ˆé—®æ¾„æ¸…é—®é¢˜
- æ¶‰åŠåŒ»ç–—/æ³•å¾‹ç­‰æ•æ„Ÿå»ºè®®æ—¶ï¼Œç»™å‡ºè¾¹ç•Œæç¤ºå¹¶å»ºè®®å¯»æ±‚ä¸“ä¸šæ„è§

å½“å‰æ—¶é—´: {current_time}
"""


class LearningAgent:
    """
    AI å­¦ä¹ æ•™ç»ƒ/ä¼´è¯» Agent
    
    åŸºäº LangChain 1.0 + LangGraph å®ç°
    - ä½¿ç”¨ create_react_agent åˆ›å»º ReAct é£æ ¼çš„æ™ºèƒ½ä½“
    - æ”¯æŒå·¥å…·è°ƒç”¨å’Œå¤šè½®å¯¹è¯
    - å†…ç½®è®°å¿†ç®¡ç†å’Œç”¨æˆ·ç”»åƒ
    - æ™ºèƒ½æ¨¡å‹è·¯ç”±ï¼šæ ¹æ®ç”¨æˆ·é…ç½®å’Œæ¶ˆæ¯ç±»å‹åŠ¨æ€é€‰æ‹©æ¨¡å‹
    """
    
    def __init__(
        self,
        user_id: str,
        mode: str = "coach",  # "coach" æˆ– "reader"
        memory: Optional[AgentMemory] = None,
    ):
        self.user_id = user_id
        self.mode = mode
        self.memory = memory or AgentMemory(user_id)
        
        # LLM å®ä¾‹ç¼“å­˜ï¼ˆæŒ‰æ¨¡å‹ç±»å‹ï¼‰
        self._llm_cache: Dict[str, ChatOpenAI] = {}
        self._current_llm: Optional[ChatOpenAI] = None
        self._current_model_info: Optional[Dict[str, Any]] = None
        
        # è·å–å·¥å…·
        self.tools = get_all_tools(user_id=user_id, memory=self.memory)
        
        # LangGraph æ£€æŸ¥ç‚¹ï¼ˆç”¨äºå¯¹è¯çŠ¶æ€æŒä¹…åŒ–ï¼‰
        self.checkpointer = MemorySaver()
        
        # Agent å®ä¾‹ï¼ˆå»¶è¿Ÿåˆ›å»ºï¼Œç­‰å¾…æ¨¡å‹é…ç½®åŠ è½½ï¼‰
        self.agent = None
    
    def _create_llm(
        self,
        model_config: Dict[str, Any],
    ) -> ChatOpenAI:
        """
        æ ¹æ®æ¨¡å‹é…ç½®åˆ›å»º LLM å®ä¾‹
        
        Args:
            model_config: æ¨¡å‹é…ç½®ï¼ŒåŒ…å« platform, model, base_url, api_key
            
        Returns:
            ChatOpenAI å®ä¾‹
        """
        import httpx
        
        # äº‘æ‰˜ç®¡ç¯å¢ƒä¸­å¯èƒ½å­˜åœ¨ SSL è¯ä¹¦é—®é¢˜ï¼Œé…ç½® HTTP å®¢æˆ·ç«¯
        http_client = None
        if IS_CLOUDRUN or DISABLE_SSL_VERIFY:
            http_client = httpx.Client(verify=False, http2=False, timeout=120.0)
        
        platform = model_config.get("platform", "deepseek")
        model = model_config.get("model", "deepseek-chat")
        base_url = model_config.get("base_url", settings.DEEPSEEK_API_BASE)
        api_key = model_config.get("api_key", "")  # API Key å¿…é¡»ä»ç”¨æˆ·é…ç½®è·å–
        
        if not api_key:
            logger.warning(f"[LearningAgent] API Key æœªé…ç½®: platform={platform}, model={model}")
        
        logger.info(f"[LearningAgent] åˆ›å»º LLM: platform={platform}, model={model}")
        
        return ChatOpenAI(
            model=model,
            api_key=api_key,
            base_url=base_url,
            temperature=0.7,
            streaming=True,
            http_client=http_client,
        )
    
    async def _get_llm_for_message(
        self,
        multimodal: Optional[Dict[str, Any]] = None,
    ) -> ChatOpenAI:
        """
        æ ¹æ®æ¶ˆæ¯ç±»å‹è·å–åˆé€‚çš„ LLM
        
        æ™ºèƒ½è·¯ç”±é€»è¾‘ï¼š
        - çº¯æ–‡æœ¬æ¶ˆæ¯ â†’ ç”¨æˆ·é…ç½®çš„æ–‡æœ¬æ¨¡å‹
        - å›¾ç‰‡æ¶ˆæ¯ â†’ ç”¨æˆ·é…ç½®çš„å¤šæ¨¡æ€/è§†è§‰æ¨¡å‹
        - è¯­éŸ³æ¶ˆæ¯ â†’ ç”¨æˆ·é…ç½®çš„è¯­éŸ³æ¨¡å‹ï¼ˆæˆ–é™çº§åˆ°æ–‡æœ¬æ¨¡å‹ï¼‰
        
        Args:
            multimodal: å¤šæ¨¡æ€æ¶ˆæ¯å­—å…¸
            
        Returns:
            ChatOpenAI å®ä¾‹
        """
        # æ£€æµ‹æ¶ˆæ¯ç±»å‹
        model_type = "text"  # é»˜è®¤æ–‡æœ¬
        
        if multimodal:
            has_image = bool(multimodal.get("image_url") or multimodal.get("image_base64"))
            has_voice = bool(multimodal.get("voice_url"))
            
            if has_image:
                model_type = "multimodal"
            elif has_voice:
                model_type = "voice"
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"{self.user_id}:{model_type}"
        if cache_key in self._llm_cache:
            logger.debug(f"[LearningAgent] ä½¿ç”¨ç¼“å­˜çš„ LLM: type={model_type}")
            return self._llm_cache[cache_key]
        
        # ä» ModelConfigService è·å–ç”¨æˆ·é…ç½®çš„æ¨¡å‹
        try:
            model_config = await ModelConfigService.get_model_for_type(
                openid=self.user_id,
                model_type=model_type,
            )
            
            is_user_config = model_config.get("is_user_config", False)
            platform = model_config.get("platform", "unknown")
            model = model_config.get("model", "unknown")
            
            if is_user_config:
                logger.info(f"[LearningAgent] ä½¿ç”¨ç”¨æˆ·é…ç½®çš„æ¨¡å‹: type={model_type}, platform={platform}, model={model}")
            else:
                logger.info(f"[LearningAgent] ä½¿ç”¨ç³»ç»Ÿé»˜è®¤æ¨¡å‹: type={model_type}, platform={platform}, model={model}")
            
            # ä¿å­˜å½“å‰æ¨¡å‹ä¿¡æ¯ï¼ˆç”¨äºæ—¥å¿—å’Œè°ƒè¯•ï¼‰
            self._current_model_info = {
                "type": model_type,
                "platform": platform,
                "model": model,
                "is_user_config": is_user_config,
            }
            
        except Exception as e:
            logger.error(f"[LearningAgent] è·å–æ¨¡å‹é…ç½®å¤±è´¥: {e}")
            # é™çº§åˆ°ç³»ç»Ÿé»˜è®¤é…ç½®ï¼ˆæ—  API Keyï¼Œä¼šåœ¨è°ƒç”¨æ—¶å¤±è´¥ï¼‰
            model_config = {
                "platform": "deepseek",
                "model": settings.DEEPSEEK_MODEL,
                "base_url": settings.DEEPSEEK_API_BASE,
                "api_key": "",  # æ— æ³•è·å–ç”¨æˆ·é…ç½®æ—¶ï¼ŒAPI Key ä¸ºç©º
                "is_user_config": False,
            }
            self._current_model_info = {
                "type": model_type,
                "platform": "deepseek",
                "model": settings.DEEPSEEK_MODEL,
                "is_user_config": False,
                "fallback_reason": str(e),
            }
        
        # åˆ›å»º LLM å®ä¾‹
        llm = self._create_llm(model_config)
        
        # ç¼“å­˜ LLM å®ä¾‹
        self._llm_cache[cache_key] = llm
        self._current_llm = llm
        
        return llm
    
    def _create_agent(self, llm: ChatOpenAI):
        """
        åˆ›å»º LangGraph ReAct Agent
        
        LangChain 1.0 æ¨èä½¿ç”¨ LangGraph çš„ create_react_agent
        è¿™æ˜¯ä¸€ä¸ªæ›´çµæ´»ã€å¯æ§çš„ Agent å®ç°æ–¹å¼
        
        æ³¨æ„ï¼šLangGraph 0.2.x+ ä¸­ state_modifier å‚æ•°å·²è¢«ç§»é™¤
        ç³»ç»Ÿæç¤ºç°åœ¨é€šè¿‡ SystemMessage åœ¨ chat() å’Œ chat_stream() ä¸­åŠ¨æ€æ·»åŠ 
        è¿™æ ·å¯ä»¥æ”¯æŒåŠ¨æ€çš„ç”¨æˆ·ç”»åƒå’Œå¯¹è¯æ‘˜è¦æ³¨å…¥
        
        Args:
            llm: ChatOpenAI å®ä¾‹ï¼ˆæ ¹æ®æ¶ˆæ¯ç±»å‹åŠ¨æ€é€‰æ‹©ï¼‰
        """
        # ä½¿ç”¨ LangGraph åˆ›å»º ReAct Agent
        # create_react_agent è¿”å›ä¸€ä¸ª CompiledGraph
        # ç³»ç»Ÿæç¤ºé€šè¿‡ _build_system_message() åŠ¨æ€æ„å»ºå¹¶ä½œä¸º SystemMessage æ·»åŠ 
        self.agent = create_react_agent(
            model=llm,
            tools=self.tools,
            checkpointer=self.checkpointer,  # å¯ç”¨å¯¹è¯çŠ¶æ€æŒä¹…åŒ–
        )
    
    def _build_multimodal_content(
        self,
        multimodal: Dict[str, Any],
    ) -> str:
        """
        æ„å»ºå¤šæ¨¡æ€æ¶ˆæ¯å†…å®¹ï¼ˆè½¬æ¢ä¸ºçº¯æ–‡æœ¬ï¼‰
        
        ç”±äº DeepSeek ä¸æ”¯æŒ image_url ç±»å‹çš„å¤šæ¨¡æ€è¾“å…¥ï¼Œ
        æˆ‘ä»¬å°†å›¾ç‰‡ä¿¡æ¯è½¬æ¢ä¸ºæ–‡æœ¬æç¤ºï¼Œè®© Agent è°ƒç”¨ recognize_image å·¥å…·å¤„ç†ã€‚
        
        Args:
            multimodal: å¤šæ¨¡æ€æ¶ˆæ¯å­—å…¸ï¼ŒåŒ…å« text, image_url, image_base64, voice_url, voice_text
            
        Returns:
            çº¯æ–‡æœ¬å­—ç¬¦ä¸²ï¼ˆDeepSeek å…¼å®¹æ ¼å¼ï¼‰
        """
        parts = []
        
        # æ–‡æœ¬éƒ¨åˆ†
        text = multimodal.get("text", "")
        if text:
            parts.append(text)
        
        # å›¾ç‰‡éƒ¨åˆ† - è½¬æ¢ä¸ºæ–‡æœ¬æç¤ºï¼Œè®© Agent è°ƒç”¨ recognize_image å·¥å…·
        image_url = multimodal.get("image_url")
        image_base64 = multimodal.get("image_base64")
        if image_url:
            # æç¤º Agent ä½¿ç”¨ recognize_image å·¥å…·å¤„ç†å›¾ç‰‡
            parts.append(f"\n\n[ç”¨æˆ·ä¸Šä¼ äº†ä¸€å¼ å›¾ç‰‡ï¼Œè¯·ä½¿ç”¨ recognize_image å·¥å…·è¯†åˆ«å›¾ç‰‡å†…å®¹]\nå›¾ç‰‡URL: {image_url}")
        elif image_base64:
            # Base64 å›¾ç‰‡ä¹Ÿè½¬æ¢ä¸ºæç¤ºï¼ˆä½† recognize_image å·¥å…·éœ€è¦ URLï¼‰
            # è¿™ç§æƒ…å†µä¸‹ï¼Œå‰ç«¯åº”è¯¥å…ˆä¸Šä¼ å›¾ç‰‡è·å– URL
            parts.append("\n\n[ç”¨æˆ·ä¸Šä¼ äº†ä¸€å¼ å›¾ç‰‡ï¼ˆBase64æ ¼å¼ï¼‰ï¼Œä½†å½“å‰æ— æ³•ç›´æ¥å¤„ç†ã€‚è¯·å‘ŠçŸ¥ç”¨æˆ·é‡æ–°ä¸Šä¼ å›¾ç‰‡ã€‚]")
        
        # è¯­éŸ³éƒ¨åˆ†ï¼ˆvoice_text ä¼˜å…ˆï¼Œå¦‚æœå‰ç«¯å·²è½¬å½•ï¼‰
        voice_text = multimodal.get("voice_text")
        
        if voice_text:
            # å‰ç«¯å·²è½¬å½•ï¼Œç›´æ¥ä½¿ç”¨è½¬å½•æ–‡æœ¬
            if not text:  # å¦‚æœæ²¡æœ‰æ–‡æœ¬ï¼Œè¯­éŸ³è½¬å½•ä½œä¸ºä¸»è¦æ–‡æœ¬
                parts.append(voice_text)
            else:  # å¦‚æœæœ‰æ–‡æœ¬ï¼Œè¯­éŸ³è½¬å½•ä½œä¸ºè¡¥å……
                parts.append(f"\n[è¯­éŸ³å†…å®¹]: {voice_text}")
        
        # åˆå¹¶æ‰€æœ‰éƒ¨åˆ†
        result = "".join(parts).strip()
        
        # å¦‚æœæ²¡æœ‰ä»»ä½•å†…å®¹ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²
        return result if result else ""
    
    async def _transcribe_voice(self, voice_url: str) -> str:
        """
        è½¬å½•è¯­éŸ³ä¸ºæ–‡æœ¬
        
        ä½¿ç”¨ OpenAI Whisper API è¿›è¡Œè¯­éŸ³è½¬æ–‡æœ¬
        
        Args:
            voice_url: è¯­éŸ³æ–‡ä»¶ URL
            
        Returns:
            è½¬å½•åçš„æ–‡æœ¬
        """
        import httpx
        from ..config import get_http_client_kwargs
        
        logger.info(f"[LearningAgent] å¼€å§‹è½¬å½•è¯­éŸ³: {voice_url[:50]}...")
        
        try:
            async with httpx.AsyncClient(**get_http_client_kwargs(60.0)) as client:
                # ä¸‹è½½éŸ³é¢‘
                audio_response = await client.get(voice_url, follow_redirects=True)
                if audio_response.status_code != 200:
                    raise ValueError(f"ä¸‹è½½éŸ³é¢‘å¤±è´¥: HTTP {audio_response.status_code}")
                
                audio_data = audio_response.content
                content_type = audio_response.headers.get("content-type", "")
                
                # æ¨æ–­æ–‡ä»¶æ ¼å¼
                if "mp3" in content_type or voice_url.endswith(".mp3"):
                    filename = "audio.mp3"
                    mime_type = "audio/mpeg"
                elif "wav" in content_type or voice_url.endswith(".wav"):
                    filename = "audio.wav"
                    mime_type = "audio/wav"
                elif "silk" in voice_url or "amr" in content_type:
                    # å¾®ä¿¡è¯­éŸ³æ ¼å¼
                    filename = "audio.silk"
                    mime_type = "audio/silk"
                else:
                    filename = "audio.mp3"
                    mime_type = "audio/mpeg"
                
                # è°ƒç”¨ OpenAI Whisper API
                transcription_url = "https://api.openai.com/v1/audio/transcriptions"
                headers = {
                    "Authorization": f"Bearer {settings.OPENAI_API_KEY}",
                }
                
                files = {
                    "file": (filename, audio_data, mime_type),
                }
                data = {
                    "model": "whisper-1",
                }
                
                response = await client.post(
                    transcription_url,
                    headers=headers,
                    files=files,
                    data=data,
                    timeout=60.0,
                )
                
                if response.status_code != 200:
                    error_text = response.text[:200] if response.text else "æœªçŸ¥é”™è¯¯"
                    raise ValueError(f"è¯­éŸ³è½¬æ–‡æœ¬å¤±è´¥: {error_text}")
                
                result = response.json()
                text = result.get("text", "")
                
                logger.info(f"[LearningAgent] è¯­éŸ³è½¬å½•æˆåŠŸ: {text[:50]}...")
                return text
                
        except Exception as e:
            logger.error(f"[LearningAgent] è¯­éŸ³è½¬å½•å¤±è´¥: {e}")
            raise
    
    async def chat(
        self,
        message: str = None,
        multimodal: Optional[Dict[str, Any]] = None,
        context: Optional[Dict[str, Any]] = None,
    ) -> str:
        """
        ä¸ Agent å¯¹è¯ï¼ˆéæµå¼ï¼‰- æ”¯æŒå¤šæ¨¡æ€
        
        Args:
            message: çº¯æ–‡æœ¬æ¶ˆæ¯ï¼ˆå‘åå…¼å®¹ï¼‰
            multimodal: å¤šæ¨¡æ€æ¶ˆæ¯ {text, image_url, image_base64, voice_url, voice_text}
            context: é¢å¤–ä¸Šä¸‹æ–‡ï¼ˆå¦‚å½“å‰é˜…è¯»çš„å†…å®¹ï¼‰
            
        Returns:
            Agent å›å¤
        """
        # æ„å»ºæ¶ˆæ¯å†…å®¹
        if multimodal:
            # å¦‚æœæœ‰è¯­éŸ³ URL ä½†æ²¡æœ‰è½¬å½•æ–‡æœ¬ï¼Œå…ˆè½¬å½•
            if multimodal.get("voice_url") and not multimodal.get("voice_text"):
                try:
                    transcribed = await self._transcribe_voice(multimodal["voice_url"])
                    multimodal["voice_text"] = transcribed
                except Exception as e:
                    logger.error(f"[LearningAgent] è¯­éŸ³è½¬å½•å¤±è´¥ï¼Œé™çº§åˆ°æ–‡æœ¬: {e}")
            
            content = self._build_multimodal_content(multimodal)
            # ç”¨äºè®°å½•çš„æ–‡æœ¬æ¶ˆæ¯
            text_for_log = multimodal.get("text") or multimodal.get("voice_text") or "[å¤šæ¨¡æ€æ¶ˆæ¯]"
        else:
            content = message
            text_for_log = message
        
        # æ™ºèƒ½æ¨¡å‹è·¯ç”±ï¼šæ ¹æ®æ¶ˆæ¯ç±»å‹è·å–åˆé€‚çš„ LLM
        llm = await self._get_llm_for_message(multimodal)
        
        # åˆ›å»º/æ›´æ–° Agentï¼ˆä½¿ç”¨é€‰å®šçš„ LLMï¼‰
        self._create_agent(llm)
        
        # å‡†å¤‡è¾“å…¥
        input_data = self._prepare_input(text_for_log, context)
        
        # é…ç½®çº¿ç¨‹ IDï¼ˆç”¨äºå¤šè½®å¯¹è¯ï¼‰
        config = {"configurable": {"thread_id": self.user_id}}
        
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = [HumanMessage(content=content)]
        
        # å¦‚æœæœ‰ç³»ç»Ÿæç¤ºï¼Œæ·»åŠ ä¸Šä¸‹æ–‡
        if input_data.get("user_profile"):
            system_content = self._build_system_message(input_data)
            messages.insert(0, SystemMessage(content=system_content))
        
        # æ‰§è¡Œ Agent
        result = await self.agent.ainvoke(
            {"messages": messages},
            config=config,
        )
        
        # æå–æœ€ç»ˆå›å¤
        output = ""
        if result.get("messages"):
            last_message = result["messages"][-1]
            if hasattr(last_message, 'content'):
                output = last_message.content
        
        # ä¿å­˜å¯¹è¯è®°å½•
        await self.memory.add_message("user", text_for_log)
        await self.memory.add_message("assistant", output)
        
        # åˆ†æå¹¶æ›´æ–°ç”¨æˆ·ç”»åƒ
        await self._analyze_and_evolve(text_for_log, {"output": output})
        
        return output
    
    async def chat_stream(
        self,
        message: str = None,
        multimodal: Optional[Dict[str, Any]] = None,
        context: Optional[Dict[str, Any]] = None,
    ) -> AsyncIterator[Dict[str, Any]]:
        """
        ä¸ Agent å¯¹è¯ï¼ˆæµå¼ï¼‰- æ”¯æŒå¤šæ¨¡æ€
        
        ä½¿ç”¨ LangGraph çš„ astream_events API å®ç°æµå¼è¾“å‡º
        è¿”å›ç»“æ„åŒ–çš„äº‹ä»¶å¯¹è±¡ï¼Œä¾¿äºå‰ç«¯è§£æå±•ç¤º
        
        Args:
            message: çº¯æ–‡æœ¬æ¶ˆæ¯ï¼ˆå‘åå…¼å®¹ï¼‰
            multimodal: å¤šæ¨¡æ€æ¶ˆæ¯ {text, image_url, image_base64, voice_url, voice_text}
            context: é¢å¤–ä¸Šä¸‹æ–‡
            
        Yields:
            ç»“æ„åŒ–çš„äº‹ä»¶å¯¹è±¡:
            - type: "text" | "tool_start" | "tool_end" | "tool_error" | "thinking" | "transcription" | "model_info"
            - content: æ–‡æœ¬å†…å®¹
            - tool_name: å·¥å…·åç§°ï¼ˆå·¥å…·äº‹ä»¶æ—¶ï¼‰
            - tool_input: å·¥å…·è¾“å…¥å‚æ•°ï¼ˆtool_start æ—¶ï¼‰
            - tool_output: å·¥å…·è¾“å‡ºç»“æœï¼ˆtool_end æ—¶ï¼‰
            - text: è¯­éŸ³è½¬å½•æ–‡æœ¬ï¼ˆtranscription äº‹ä»¶æ—¶ï¼‰
            - model_info: å½“å‰ä½¿ç”¨çš„æ¨¡å‹ä¿¡æ¯ï¼ˆmodel_info äº‹ä»¶æ—¶ï¼‰
        """
        # æ„å»ºæ¶ˆæ¯å†…å®¹
        if multimodal:
            # å¦‚æœæœ‰è¯­éŸ³ URL ä½†æ²¡æœ‰è½¬å½•æ–‡æœ¬ï¼Œå…ˆè½¬å½•
            if multimodal.get("voice_url") and not multimodal.get("voice_text"):
                try:
                    transcribed = await self._transcribe_voice(multimodal["voice_url"])
                    multimodal["voice_text"] = transcribed
                    # å‘é€è½¬å½•äº‹ä»¶
                    yield {"type": "transcription", "text": transcribed}
                except Exception as e:
                    logger.error(f"[LearningAgent] è¯­éŸ³è½¬å½•å¤±è´¥: {e}")
                    yield {"type": "error", "error": f"è¯­éŸ³è½¬å½•å¤±è´¥: {str(e)}"}
                    return
            
            content = self._build_multimodal_content(multimodal)
            # ç”¨äºè®°å½•çš„æ–‡æœ¬æ¶ˆæ¯
            text_for_log = multimodal.get("text") or multimodal.get("voice_text") or "[å¤šæ¨¡æ€æ¶ˆæ¯]"
        else:
            content = message
            text_for_log = message
        
        # æ™ºèƒ½æ¨¡å‹è·¯ç”±ï¼šæ ¹æ®æ¶ˆæ¯ç±»å‹è·å–åˆé€‚çš„ LLM
        llm = await self._get_llm_for_message(multimodal)
        
        # åˆ›å»º/æ›´æ–° Agentï¼ˆä½¿ç”¨é€‰å®šçš„ LLMï¼‰
        self._create_agent(llm)
        
        # å‘é€æ¨¡å‹ä¿¡æ¯äº‹ä»¶ï¼ˆè®©å‰ç«¯çŸ¥é“ä½¿ç”¨äº†å“ªä¸ªæ¨¡å‹ï¼‰
        if self._current_model_info:
            yield {
                "type": "model_info",
                "model_info": self._current_model_info,
            }
        
        # å‡†å¤‡è¾“å…¥
        input_data = self._prepare_input(text_for_log, context)
        
        # é…ç½®
        config = {"configurable": {"thread_id": self.user_id}}
        
        # æ„å»ºæ¶ˆæ¯
        messages = [HumanMessage(content=content)]
        if input_data.get("user_profile"):
            system_content = self._build_system_message(input_data)
            messages.insert(0, SystemMessage(content=system_content))
        
        full_response = ""
        current_tool_calls = {}  # è¿½è¸ªå½“å‰å·¥å…·è°ƒç”¨
        
        # ä½¿ç”¨ astream_events è¿›è¡Œæµå¼å¤„ç†
        async for event in self.agent.astream_events(
            {"messages": messages},
            config=config,
            version="v2",
        ):
            kind = event["event"]
            
            # å¤„ç† LLM æµå¼è¾“å‡º
            if kind == "on_chat_model_stream":
                chunk = event.get("data", {}).get("chunk")
                if chunk and hasattr(chunk, 'content') and chunk.content:
                    content_chunk = chunk.content
                    full_response += content_chunk
                    yield {"type": "text", "content": content_chunk}
            
            # å¤„ç†å·¥å…·è°ƒç”¨å¼€å§‹
            elif kind == "on_tool_start":
                tool_name = event.get("name", "unknown")
                tool_input = event.get("data", {}).get("input", {})
                run_id = event.get("run_id", "")
                
                # è·å–å·¥å…·çš„ä¸­æ–‡åç§°å’Œæè¿°
                tool_info = self._get_tool_display_info(tool_name)
                
                current_tool_calls[run_id] = {
                    "name": tool_name,
                    "display_name": tool_info["display_name"],
                    "description": tool_info["description"],
                    "icon": tool_info["icon"],
                    "input": tool_input,
                }
                
                yield {
                    "type": "tool_start",
                    "tool_name": tool_name,
                    "display_name": tool_info["display_name"],
                    "description": tool_info["description"],
                    "icon": tool_info["icon"],
                    "tool_input": tool_input,
                    "run_id": run_id,
                }
            
            # å¤„ç†å·¥å…·è°ƒç”¨ç»“æŸ
            elif kind == "on_tool_end":
                run_id = event.get("run_id", "")
                tool_output = event.get("data", {}).get("output", "")
                
                tool_call_info = current_tool_calls.get(run_id, {})
                tool_name = tool_call_info.get("name", "unknown")
                
                # è§£æå·¥å…·è¾“å‡º
                parsed_output = self._parse_tool_output(tool_output)
                
                yield {
                    "type": "tool_end",
                    "tool_name": tool_name,
                    "display_name": tool_call_info.get("display_name", tool_name),
                    "icon": tool_call_info.get("icon", "ğŸ”§"),
                    "tool_output": parsed_output,
                    "success": parsed_output.get("success", True),
                    "run_id": run_id,
                }
                
                # æ¸…ç†å·²å®Œæˆçš„å·¥å…·è°ƒç”¨
                if run_id in current_tool_calls:
                    del current_tool_calls[run_id]
            
            # å¤„ç†å·¥å…·è°ƒç”¨é”™è¯¯
            elif kind == "on_tool_error":
                run_id = event.get("run_id", "")
                error = event.get("data", {}).get("error", "æœªçŸ¥é”™è¯¯")
                
                tool_call_info = current_tool_calls.get(run_id, {})
                
                yield {
                    "type": "tool_error",
                    "tool_name": tool_call_info.get("name", "unknown"),
                    "display_name": tool_call_info.get("display_name", "å·¥å…·"),
                    "icon": tool_call_info.get("icon", "ğŸ”§"),
                    "error": str(error),
                    "run_id": run_id,
                }
        
        # ä¿å­˜å¯¹è¯è®°å½•
        await self.memory.add_message("user", text_for_log)
        await self.memory.add_message("assistant", full_response)
        
        # å¼‚æ­¥åˆ†æå¹¶è¿›åŒ–
        await self._analyze_and_evolve(text_for_log, {"output": full_response})
    
    def _get_tool_display_info(self, tool_name: str) -> Dict[str, str]:
        """è·å–å·¥å…·çš„æ˜¾ç¤ºä¿¡æ¯ï¼ˆä¸­æ–‡åç§°ã€æè¿°ã€å›¾æ ‡ï¼‰"""
        tool_info_map = {
            # å­¦ä¹ è®¡åˆ’ç›¸å…³
            "create_learning_plan": {"display_name": "åˆ›å»ºå­¦ä¹ è®¡åˆ’", "description": "ä¸ºä½ åˆ¶å®šä¸ªæ€§åŒ–å­¦ä¹ è®¡åˆ’", "icon": "ğŸ“‹"},
            "generate_daily_tasks": {"display_name": "ç”Ÿæˆä»Šæ—¥ä»»åŠ¡", "description": "ç”Ÿæˆæ¯æ—¥å­¦ä¹ ä»»åŠ¡", "icon": "ğŸ“"},
            
            # æœç´¢ç›¸å…³
            "search_resources": {"display_name": "è”ç½‘æœç´¢", "description": "åœ¨ç½‘ä¸Šæœç´¢ç›¸å…³èµ„æ–™", "icon": "ğŸ”"},
            "search_learning_materials": {"display_name": "æœç´¢å­¦ä¹ èµ„æ–™", "description": "æœç´¢å­¦ä¹ ç›¸å…³ææ–™", "icon": "ğŸ“š"},
            
            # å›¾ç‰‡è¯†åˆ«
            "recognize_image": {"display_name": "å›¾ç‰‡è¯†åˆ«", "description": "è¯†åˆ«å›¾ç‰‡ä¸­çš„å†…å®¹", "icon": "ğŸ–¼ï¸"},
            
            # ä»»åŠ¡ç®¡ç†
            "get_today_tasks": {"display_name": "è·å–ä»Šæ—¥ä»»åŠ¡", "description": "æŸ¥çœ‹ä»Šå¤©çš„å­¦ä¹ ä»»åŠ¡", "icon": "ğŸ“‹"},
            "complete_task": {"display_name": "å®Œæˆä»»åŠ¡", "description": "æ ‡è®°ä»»åŠ¡ä¸ºå·²å®Œæˆ", "icon": "âœ…"},
            "get_task_progress": {"display_name": "ä»»åŠ¡è¿›åº¦", "description": "æŸ¥çœ‹ä»»åŠ¡å®Œæˆè¿›åº¦", "icon": "ğŸ“Š"},
            "suggest_task_adjustment": {"display_name": "è°ƒæ•´å»ºè®®", "description": "å»ºè®®è°ƒæ•´ä»»åŠ¡å®‰æ’", "icon": "ğŸ’¡"},
            
            # æ‰“å¡ç³»ç»Ÿ
            "do_checkin": {"display_name": "å­¦ä¹ æ‰“å¡", "description": "æ‰§è¡Œå­¦ä¹ æ‰“å¡ç­¾åˆ°", "icon": "âœ¨"},
            "get_checkin_status": {"display_name": "æ‰“å¡çŠ¶æ€", "description": "æŸ¥çœ‹æ‰“å¡ç»Ÿè®¡", "icon": "ğŸ“ˆ"},
            "get_badges": {"display_name": "æˆå°±å¾½ç« ", "description": "æŸ¥çœ‹è·å¾—çš„å¾½ç« ", "icon": "ğŸ…"},
            
            # ç•ªèŒ„ä¸“æ³¨
            "get_focus_stats": {"display_name": "ä¸“æ³¨ç»Ÿè®¡", "description": "æŸ¥çœ‹ä¸“æ³¨æ—¶é—´ç»Ÿè®¡", "icon": "ğŸ…"},
            "suggest_focus_plan": {"display_name": "ä¸“æ³¨è®¡åˆ’", "description": "å»ºè®®ä¸“æ³¨è®¡åˆ’å®‰æ’", "icon": "â±ï¸"},
            
            # é”™é¢˜æœ¬
            "get_mistakes": {"display_name": "é”™é¢˜åˆ—è¡¨", "description": "æŸ¥çœ‹é”™é¢˜æœ¬", "icon": "ğŸ“•"},
            "add_mistake": {"display_name": "æ·»åŠ é”™é¢˜", "description": "æ·»åŠ æ–°é”™é¢˜", "icon": "â•"},
            "analyze_mistake": {"display_name": "é”™é¢˜åˆ†æ", "description": "AIåˆ†æé”™é¢˜åŸå› ", "icon": "ğŸ”¬"},
            "generate_review_questions": {"display_name": "ç”Ÿæˆç»ƒä¹ é¢˜", "description": "ç”Ÿæˆå¤ä¹ é¢˜ç›®", "icon": "ğŸ“"},
            "mark_mistake_mastered": {"display_name": "æ ‡è®°å·²æŒæ¡", "description": "æ ‡è®°é”™é¢˜ä¸ºå·²æŒæ¡", "icon": "ğŸ¯"},
            
            # ç»Ÿè®¡åˆ†æ
            "get_learning_stats": {"display_name": "å­¦ä¹ ç»Ÿè®¡", "description": "è·å–å­¦ä¹ æ•°æ®ç»Ÿè®¡", "icon": "ğŸ“Š"},
            "get_ranking": {"display_name": "æ’è¡Œæ¦œ", "description": "æŸ¥çœ‹å­¦ä¹ æ’è¡Œæ¦œ", "icon": "ğŸ†"},
            "get_achievement_rate": {"display_name": "è¾¾æˆç‡", "description": "æŸ¥çœ‹ç›®æ ‡è¾¾æˆç‡", "icon": "ğŸ¯"},
            "analyze_learning_pattern": {"display_name": "å­¦ä¹ åˆ†æ", "description": "åˆ†æå­¦ä¹ æ¨¡å¼", "icon": "ğŸ“ˆ"},
            "get_calendar_data": {"display_name": "æ—¥å†æ•°æ®", "description": "æŸ¥çœ‹æ—¥å†å­¦ä¹ è¯¦æƒ…", "icon": "ğŸ“…"},
            "analyze_learning_status": {"display_name": "çŠ¶æ€åˆ†æ", "description": "åˆ†ææ•´ä½“å­¦ä¹ çŠ¶æ€", "icon": "ğŸ’¡"},
            
            # ç”¨æˆ·ç”»åƒ
            "update_user_profile": {"display_name": "æ›´æ–°ç”»åƒ", "description": "æ›´æ–°ç”¨æˆ·å­¦ä¹ ç”»åƒ", "icon": "ğŸ‘¤"},
            "get_user_stats": {"display_name": "ç”¨æˆ·ç»Ÿè®¡", "description": "è·å–ç”¨æˆ·ç»Ÿè®¡ä¿¡æ¯", "icon": "ğŸ“‹"},
            
            # æ–‡æ¡£ä¼´è¯»
            "get_documents": {"display_name": "æ–‡æ¡£åˆ—è¡¨", "description": "è·å–å­¦ä¹ æ–‡æ¡£åˆ—è¡¨", "icon": "ğŸ“š"},
            "search_documents": {"display_name": "æœç´¢æ–‡æ¡£", "description": "æœç´¢å­¦ä¹ æ–‡æ¡£", "icon": "ğŸ”"},
            "get_document_stats": {"display_name": "æ–‡æ¡£ç»Ÿè®¡", "description": "è·å–æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯", "icon": "ğŸ“Š"},
            "get_recent_documents": {"display_name": "æœ€è¿‘æ–‡æ¡£", "description": "è·å–æœ€è¿‘é˜…è¯»çš„æ–‡æ¡£", "icon": "ğŸ“–"},
        }
        
        return tool_info_map.get(tool_name, {
            "display_name": tool_name,
            "description": "æ‰§è¡Œæ“ä½œ",
            "icon": "ğŸ”§"
        })
    
    def _parse_tool_output(self, output: Any) -> Dict[str, Any]:
        """
        è§£æå·¥å…·è¾“å‡ºï¼Œè½¬æ¢ä¸ºç»“æ„åŒ–æ•°æ®
        
        LangChain å·¥å…·å¯èƒ½è¿”å›å¤šç§æ ¼å¼ï¼š
        1. å­—ç¬¦ä¸²
        2. dict
        3. ToolMessage å¯¹è±¡ï¼ˆæœ‰ content å±æ€§ï¼‰
        4. ToolMessage çš„å­—ç¬¦ä¸²è¡¨ç¤º "content='...' name='...' tool_call_id='...'"
        """
        # å¦‚æœæ˜¯ LangChain çš„æ¶ˆæ¯å¯¹è±¡ï¼Œæå– content
        if hasattr(output, 'content'):
            content = output.content
            return {"success": True, "message": content}
        
        if isinstance(output, str):
            # æ£€æŸ¥æ˜¯å¦æ˜¯ ToolMessage çš„å­—ç¬¦ä¸²è¡¨ç¤º
            # æ ¼å¼ç±»ä¼¼: content='...' name='...' tool_call_id='...'
            if output.startswith("content='") or "content='" in output:
                try:
                    # æå– content å­—æ®µçš„å€¼
                    import re
                    # åŒ¹é… content='...' æˆ– content="..."
                    match = re.search(r"content=['\"](.+?)['\"](?:\s+name=|\s*$)", output, re.DOTALL)
                    if match:
                        content = match.group(1)
                        # å¤„ç†è½¬ä¹‰å­—ç¬¦
                        content = content.replace('\\n', '\n').replace("\\'", "'").replace('\\"', '"')
                        return {"success": True, "message": content}
                except Exception:
                    pass
            
            # å°è¯•è§£æä¸º JSON
            try:
                return json.loads(output)
            except json.JSONDecodeError:
                return {"success": True, "message": output}
        
        elif isinstance(output, dict):
            # å¦‚æœ dict åŒ…å« content å­—æ®µï¼Œæå–å‡ºæ¥
            if 'content' in output:
                return {"success": True, "message": output['content']}
            return output
        
        else:
            return {"success": True, "data": str(output)}
    
    def _build_system_message(self, input_data: Dict[str, Any]) -> str:
        """æ„å»ºç³»ç»Ÿæ¶ˆæ¯å†…å®¹"""
        template = (
            LEARNING_COACH_PROMPT if self.mode == "coach"
            else READING_COMPANION_PROMPT
        )
        
        return template.format(
            user_profile=input_data.get("user_profile", "æ–°ç”¨æˆ·"),
            conversation_summary=input_data.get("conversation_summary", "æ–°å¯¹è¯"),
            current_time=input_data.get("current_time", ""),
            reading_context=input_data.get("reading_context", "æ— "),
        )
    
    def _prepare_input(
        self,
        message: str,
        context: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """å‡†å¤‡ Agent è¾“å…¥"""
        from datetime import datetime
        
        # è·å–ç”¨æˆ·ç”»åƒ
        user_profile = self.memory.get_user_profile_summary()
        
        # è·å–å¯¹è¯æ‘˜è¦
        conversation_summary = self.memory.get_conversation_summary()
        
        # è·å–èŠå¤©å†å²
        chat_history = self.memory.get_chat_history(limit=10)
        
        # æ„å»ºè¾“å…¥
        input_data = {
            "input": message,
            "chat_history": chat_history,
            "user_profile": user_profile,
            "conversation_summary": conversation_summary,
            "current_time": datetime.now().strftime("%Y-%m-%d %H:%M"),
        }
        
        # æ·»åŠ æ¨¡å¼ç‰¹å®šçš„ä¸Šä¸‹æ–‡
        if self.mode == "reader" and context:
            input_data["reading_context"] = json.dumps(
                context, ensure_ascii=False, indent=2
            )
        else:
            input_data["reading_context"] = "æ— "
        
        return input_data
    
    async def _analyze_and_evolve(
        self,
        user_message: str,
        result: Dict[str, Any],
    ):
        """
        åˆ†æå¯¹è¯å¹¶æ›´æ–°ç”¨æˆ·ç”»åƒï¼ˆè¿›åŒ–æœºåˆ¶ï¼‰
        
        è¿™æ˜¯ Agent è‡ªæˆ‘è¿›åŒ–çš„æ ¸å¿ƒï¼š
        1. åˆ†æç”¨æˆ·çš„å­¦ä¹ åå¥½
        2. è¯†åˆ«ç”¨æˆ·çš„çŸ¥è¯†æ°´å¹³
        3. è®°å½•ç”¨æˆ·çš„å…´è¶£é¢†åŸŸ
        4. ä¼˜åŒ–äº¤äº’ç­–ç•¥
        """
        try:
            # æå–å…³é”®ä¿¡æ¯
            insights = await self._extract_insights(user_message, result)
            
            if insights:
                # æ›´æ–°ç”¨æˆ·ç”»åƒ
                await self.memory.update_user_profile(insights)
                
        except Exception as e:
            # è¿›åŒ–å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
            print(f"è¿›åŒ–åˆ†æå¤±è´¥: {e}")
    
    async def _extract_insights(
        self,
        user_message: str,
        result: Dict[str, Any],
    ) -> Optional[Dict[str, Any]]:
        """ä»å¯¹è¯ä¸­æå–ç”¨æˆ·æ´å¯Ÿ"""
        # ä½¿ç”¨å½“å‰ LLM åˆ†æå¯¹è¯ï¼ˆå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤ï¼‰
        llm = self._current_llm
        if not llm:
            # è·å–é»˜è®¤æ–‡æœ¬æ¨¡å‹
            llm = await self._get_llm_for_message(None)
        
        analysis_prompt = f"""åˆ†æä»¥ä¸‹å¯¹è¯ï¼Œæå–ç”¨æˆ·å­¦ä¹ ç›¸å…³çš„æ´å¯Ÿï¼š

ç”¨æˆ·æ¶ˆæ¯: {user_message}
åŠ©æ‰‹å›å¤: {result.get('output', '')[:500]}

è¯·ä»¥ JSON æ ¼å¼è¿”å›æ´å¯Ÿï¼ˆå¦‚æœæ²¡æœ‰æœ‰ä»·å€¼çš„æ´å¯Ÿè¿”å› nullï¼‰ï¼š
{{
    "learning_style": "ç”¨æˆ·çš„å­¦ä¹ é£æ ¼åå¥½ï¼ˆå¦‚æœ‰ï¼‰",
    "knowledge_level": "ç”¨æˆ·åœ¨æŸé¢†åŸŸçš„çŸ¥è¯†æ°´å¹³ï¼ˆå¦‚æœ‰ï¼‰",
    "interests": ["ç”¨æˆ·æ„Ÿå…´è¶£çš„ä¸»é¢˜ï¼ˆå¦‚æœ‰ï¼‰"],
    "pain_points": ["ç”¨æˆ·é‡åˆ°çš„å›°éš¾ï¼ˆå¦‚æœ‰ï¼‰"],
    "preferences": "ç”¨æˆ·çš„äº¤äº’åå¥½ï¼ˆå¦‚æœ‰ï¼‰"
}}

åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚å¦‚æœæ²¡æœ‰æœ‰ä»·å€¼çš„æ´å¯Ÿï¼Œè¿”å› nullã€‚
"""
        
        try:
            response = await llm.ainvoke([HumanMessage(content=analysis_prompt)])
            content = response.content.strip()
            
            if content and content != "null":
                # å°è¯•è§£æ JSON
                if content.startswith("```"):
                    content = content.split("```")[1]
                    if content.startswith("json"):
                        content = content[4:]
                
                return json.loads(content)
        except Exception:
            pass
        
        return None
    
    async def get_suggestions(self) -> List[str]:
        """
        æ ¹æ®ç”¨æˆ·ç”»åƒç”Ÿæˆä¸ªæ€§åŒ–å»ºè®®
        
        è¿™æ˜¯è¿›åŒ–æœºåˆ¶çš„ä½“ç°ä¹‹ä¸€ï¼šæ ¹æ®ç§¯ç´¯çš„ç”¨æˆ·æ•°æ®æä¾›æ›´å¥½çš„å»ºè®®
        """
        profile = self.memory.get_user_profile()
        
        if not profile:
            return [
                "å¼€å§‹åˆ¶å®šä½ çš„å­¦ä¹ è®¡åˆ’å§ï¼",
                "å‘Šè¯‰æˆ‘ä½ æƒ³å­¦ä»€ä¹ˆ",
                "ä¸Šä¼ ä¸€å¼ é¢˜ç›®å›¾ç‰‡ï¼Œæˆ‘æ¥å¸®ä½ è§£ç­”",
            ]
        
        # ä½¿ç”¨å½“å‰ LLM æˆ–è·å–é»˜è®¤æ–‡æœ¬æ¨¡å‹
        llm = self._current_llm
        if not llm:
            llm = await self._get_llm_for_message(None)
        
        # æ ¹æ®ç”¨æˆ·ç”»åƒç”Ÿæˆä¸ªæ€§åŒ–å»ºè®®
        suggestions_prompt = f"""æ ¹æ®ä»¥ä¸‹ç”¨æˆ·ç”»åƒï¼Œç”Ÿæˆ3æ¡ä¸ªæ€§åŒ–çš„å­¦ä¹ å»ºè®®ï¼š

ç”¨æˆ·ç”»åƒ:
{json.dumps(profile, ensure_ascii=False, indent=2)}

è¦æ±‚ï¼š
1. å»ºè®®è¦å…·ä½“å¯æ‰§è¡Œ
2. ä¸ç”¨æˆ·çš„å­¦ä¹ ç›®æ ‡ç›¸å…³
3. è€ƒè™‘ç”¨æˆ·çš„å­¦ä¹ é£æ ¼

ä»¥ JSON æ•°ç»„æ ¼å¼è¿”å›ï¼Œæ¯æ¡å»ºè®®ä¸è¶…è¿‡20ä¸ªå­—ï¼š
["å»ºè®®1", "å»ºè®®2", "å»ºè®®3"]
"""
        
        try:
            response = await llm.ainvoke([HumanMessage(content=suggestions_prompt)])
            return json.loads(response.content.strip())
        except Exception:
            return ["ç»§ç»­åŠ æ²¹å­¦ä¹ ï¼", "ä¿æŒå­¦ä¹ èŠ‚å¥", "æœ‰é—®é¢˜éšæ—¶é—®æˆ‘"]
