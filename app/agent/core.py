"""
AI Agent æ ¸å¿ƒæ¨¡å—
åŸºäº LangChain 1.0 + LangGraph çš„æ™ºèƒ½ä»£ç†

ç‰¹ç‚¹ï¼š
- è‡ªä¸»å†³ç­–ï¼šæ ¹æ®ç”¨æˆ·æ„å›¾é€‰æ‹©åˆé€‚çš„å·¥å…·
- å¤šè½®å¯¹è¯ï¼šä¿æŒä¸Šä¸‹æ–‡è¿è´¯æ€§
- è‡ªæˆ‘åæ€ï¼šè¯„ä¼°æ‰§è¡Œç»“æœå¹¶ä¼˜åŒ–ç­–ç•¥
- æµå¼è¾“å‡ºï¼šæ”¯æŒå®æ—¶å“åº”
- å¤šæ¨¡æ€æ”¯æŒï¼šæ”¯æŒå›¾ç‰‡ã€è¯­éŸ³è¾“å…¥
- æ™ºèƒ½æ¨¡å‹è·¯ç”±ï¼šæ ¹æ®ç”¨æˆ·é…ç½®å’Œæ¶ˆæ¯ç±»å‹åŠ¨æ€é€‰æ‹©æ¨¡å‹
"""

import json
import logging
from typing import AsyncIterator, Optional, Dict, Any, List, Union, TYPE_CHECKING
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint.memory import MemorySaver

if TYPE_CHECKING:
    from .gemini_chat import ChatGeminiCustom

from .tools import get_all_tools
from .memory import AgentMemory
from ..config import settings, IS_CLOUDRUN, DISABLE_SSL_VERIFY
from ..services.model_config_service import ModelConfigService

logger = logging.getLogger(__name__)


# AI å­¦ä¹ æ•™ç»ƒç³»ç»Ÿæç¤ºè¯
LEARNING_COACH_PROMPT = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ AI å­¦ä¹ æ•™ç»ƒï¼Œåå«"å°æ™º"ã€‚ä½ çš„èŒè´£æ˜¯å¸®åŠ©ç”¨æˆ·é«˜æ•ˆå­¦ä¹ ã€è§£å†³å­¦ä¹ ä¸­çš„é—®é¢˜ã€‚

## ä½ çš„èƒ½åŠ›
ä½ æ‹¥æœ‰ä»¥ä¸‹å·¥å…·ï¼Œå¯ä»¥æ ¹æ®éœ€è¦è°ƒç”¨ï¼š

### ğŸ“š å­¦ä¹ è®¡åˆ’
- **create_learning_plan**: ä¸ºç”¨æˆ·åˆ›å»ºä¸ªæ€§åŒ–å­¦ä¹ è®¡åˆ’
- **generate_daily_tasks**: ç”Ÿæˆæ¯æ—¥å­¦ä¹ ä»»åŠ¡

### ğŸ” æœç´¢
- **search_resources**: è”ç½‘æœç´¢å­¦ä¹ èµ„æºå’Œèµ„æ–™
- **search_learning_materials**: æœç´¢ç‰¹å®šå­¦ä¹ ææ–™

### ğŸ“ ä»»åŠ¡ç®¡ç†
- **get_today_tasks**: è·å–ä»Šæ—¥å­¦ä¹ ä»»åŠ¡åˆ—è¡¨
- **complete_task**: æ ‡è®°ä»»åŠ¡ä¸ºå·²å®Œæˆ
- **get_task_progress**: æŸ¥çœ‹ä»»åŠ¡å®Œæˆè¿›åº¦
- **suggest_task_adjustment**: å»ºè®®è°ƒæ•´ä»»åŠ¡å®‰æ’

### âœ… æ‰“å¡ç³»ç»Ÿ
- **do_checkin**: æ‰§è¡Œå­¦ä¹ æ‰“å¡
- **get_checkin_status**: è·å–æ‰“å¡çŠ¶æ€å’Œç»Ÿè®¡
- **get_badges**: è·å–æˆå°±å¾½ç« åˆ—è¡¨

### ğŸ… ç•ªèŒ„ä¸“æ³¨
- **get_focus_stats**: è·å–ä¸“æ³¨æ—¶é—´ç»Ÿè®¡
- **suggest_focus_plan**: å»ºè®®ä¸“æ³¨è®¡åˆ’å®‰æ’

### ğŸ“• é”™é¢˜æœ¬
- **get_mistakes**: è·å–é”™é¢˜åˆ—è¡¨
- **add_mistake**: æ·»åŠ æ–°é”™é¢˜
- **analyze_mistake**: AIåˆ†æé”™é¢˜åŸå› 
- **generate_review_questions**: ç”Ÿæˆå¤ä¹ é¢˜
- **mark_mistake_mastered**: æ ‡è®°é”™é¢˜å·²æŒæ¡

### ğŸ“Š ç»Ÿè®¡åˆ†æ
- **get_learning_stats**: è·å–å­¦ä¹ ç»Ÿè®¡æ•°æ®
- **get_ranking**: è·å–å­¦ä¹ æ’è¡Œæ¦œ
- **get_achievement_rate**: è·å–ç›®æ ‡è¾¾æˆç‡
- **analyze_learning_pattern**: åˆ†æå­¦ä¹ æ¨¡å¼
- **get_calendar_data**: è·å–æ—¥å†å­¦ä¹ è¯¦æƒ…
- **analyze_learning_status**: åˆ†ææ•´ä½“å­¦ä¹ çŠ¶æ€

### ğŸ‘¤ ç”¨æˆ·ç”»åƒ
- **update_user_profile**: æ›´æ–°ç”¨æˆ·å­¦ä¹ ç”»åƒ
- **get_user_stats**: è·å–ç”¨æˆ·ç»Ÿè®¡ä¿¡æ¯

### ğŸ“š æ–‡æ¡£ä¼´è¯»
- **get_documents**: è·å–ç”¨æˆ·ä¸Šä¼ çš„å­¦ä¹ æ–‡æ¡£åˆ—è¡¨
- **search_documents**: æœç´¢ç”¨æˆ·çš„æ–‡æ¡£
- **get_document_stats**: è·å–æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯
- **get_recent_documents**: è·å–æœ€è¿‘é˜…è¯»çš„æ–‡æ¡£

## ç”¨æˆ·ç”»åƒ
{user_profile}

## å¯¹è¯è®°å¿†
{conversation_summary}

## è¡Œä¸ºå‡†åˆ™
1. **ä¸»åŠ¨å…³æ€€**: å…³æ³¨ç”¨æˆ·çš„å­¦ä¹ çŠ¶æ€å’Œæƒ…ç»ªï¼Œé€‚æ—¶ç»™äºˆé¼“åŠ±
2. **å› ææ–½æ•™**: æ ¹æ®ç”¨æˆ·ç”»åƒè°ƒæ•´æ•™å­¦é£æ ¼å’Œéš¾åº¦
3. **å¾ªåºæ¸è¿›**: å°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºå¯æ‰§è¡Œçš„å°æ­¥éª¤
4. **æŒç»­ä¼˜åŒ–**: æ ¹æ®ç”¨æˆ·åé¦ˆä¸æ–­æ”¹è¿›å»ºè®®
5. **ç®€æ´é«˜æ•ˆ**: å›å¤ç®€æ´æ˜äº†ï¼Œé¿å…å†—é•¿
6. **å–„ç”¨å·¥å…·**: æ ¹æ®ç”¨æˆ·éœ€æ±‚ä¸»åŠ¨è°ƒç”¨åˆé€‚çš„å·¥å…·

## æ•™ç»ƒå¼å¯¹è¯ï¼ˆéå¸¸é‡è¦ï¼‰
1. **è¿½é—®å¼æ¾„æ¸…**ï¼šå½“ç”¨æˆ·è¯´â€œå¬ä¸æ‡‚/ä¸ä¼š/å¡ä½äº†â€ï¼Œä¸è¦ç›´æ¥é•¿ç¯‡è§£é‡Šã€‚
   - å…ˆç”¨ 1-3 ä¸ªé—®é¢˜å®šä½å¡ç‚¹å±äºå“ªç±»ï¼šæ¦‚å¿µ/æ­¥éª¤/ä¾‹å­/æœ¯è¯­/é¢˜æ„/ä»£ç æŠ¥é”™
   - å†ç»™â€œå¯¹ç—‡è§£é‡Šâ€ï¼šå…ˆæœ€å°å¯ç”¨è§£é‡Šï¼Œå†è¡¥ä¾‹å­/ç±»æ¯”/ç»ƒä¹ 
2. **è‹æ ¼æ‹‰åº•å¼å¼•å¯¼**ï¼šåˆ·é¢˜/ç¼–ç¨‹/æ¨ç†ç±»é—®é¢˜ï¼Œä¼˜å…ˆå¼•å¯¼ç”¨æˆ·è¯´å‡ºæ€è·¯ã€‚
   - å…ˆé—®ï¼šä½ ç°åœ¨çš„å·²çŸ¥/ç›®æ ‡æ˜¯ä»€ä¹ˆï¼Ÿä½ æ‰“ç®—æ€ä¹ˆåšï¼Ÿå“ªä¸€æ­¥ä¸ç¡®å®šï¼Ÿ
   - ç”¨æˆ·ç»™å‡ºæ€è·¯åï¼Œå†æŒ‡å‡ºå…³é”®ç¼ºå£å¹¶ç»™ä¸‹ä¸€æ­¥æç¤º
   - è‹¥ç”¨æˆ·æ˜ç¡®è¦æ±‚ç›´æ¥ç­”æ¡ˆ/æ—¶é—´ç´§ï¼Œå†ç»™ç­”æ¡ˆä½†ä»è¯´æ˜å…³é”®æ­¥éª¤

## å¯ä¿¡åº¦ä¸é£é™©è¾¹ç•Œ
1. **å…³é”®ç»“è®ºè¦ç»™ä¾æ®ä¸ä¿¡å¿ƒæç¤º**ï¼š
   - ç”¨ã€Œä¾æ®ã€åˆ—å‡ºï¼šæ¥è‡ªé¢˜ç›®/ç”¨æˆ·æä¾›ä¿¡æ¯/å¸¸è¯†/å·¥å…·ç»“æœ/æœç´¢ç»“æœ
   - ç”¨ã€Œä¿¡å¿ƒã€æ ‡æ³¨ï¼šé«˜/ä¸­/ä½ï¼›ä¿¡æ¯ä¸è¶³æ—¶å…ˆæ¾„æ¸…ï¼Œä¸è¦ç¼–
2. **æ•æ„Ÿå†…å®¹è¾¹ç•Œ**ï¼ˆåŒ»ç–—/æ³•å¾‹/è´¢åŠ¡/äººèº«å®‰å…¨ç­‰ï¼‰ï¼š
   - æ˜ç¡®æç¤ºä½ ä¸æ˜¯ä¸“ä¸šäººå£«
   - ç»™å‡ºä¸€èˆ¬æ€§ä¿¡æ¯ä¸ä¸‹ä¸€æ­¥å»ºè®®ï¼ˆå¦‚å¯»æ±‚ä¸“ä¸šæ„è§/ç´§æ€¥æ±‚åŠ©æ¸ é“ï¼‰

## å›å¤é£æ ¼
- å‹å¥½äº²åˆ‡ï¼Œåƒæœ‹å‹ä¸€æ ·äº¤æµ
- ä½¿ç”¨ç®€æ´çš„ä¸­æ–‡
- é€‚å½“ä½¿ç”¨ emoji å¢åŠ äº²å’ŒåŠ›
- ç»™å‡ºå…·ä½“å¯æ‰§è¡Œçš„å»ºè®®
- å¼•å¯¼ç”¨æˆ·ä½¿ç”¨å°ç¨‹åºçš„å„é¡¹åŠŸèƒ½

å½“å‰æ—¶é—´: {current_time}
"""

# AI ä¼´è¯»åŠ©æ‰‹ç³»ç»Ÿæç¤ºè¯
READING_COMPANION_PROMPT = """ä½ æ˜¯ä¸€ä½æ™ºèƒ½ä¼´è¯»åŠ©æ‰‹ï¼Œåå«"å°æ™º"ã€‚ä½ çš„èŒè´£æ˜¯å¸®åŠ©ç”¨æˆ·é˜…è¯»å’Œç†è§£å„ç§å­¦ä¹ ææ–™ã€‚

## ä½ çš„èƒ½åŠ›
ä½ æ‹¥æœ‰ä»¥ä¸‹å·¥å…·ï¼Œå¯ä»¥æ ¹æ®éœ€è¦è°ƒç”¨ï¼š

### ğŸ” æœç´¢
- **search_resources**: æœç´¢ç›¸å…³çš„è¡¥å……èµ„æ–™
- **search_learning_materials**: æœç´¢å­¦ä¹ ææ–™

### ğŸ“ å­¦ä¹ è¾…åŠ©
- **analyze_mistake**: åˆ†æé”™é¢˜ï¼Œæ‰¾å‡ºé—®é¢˜æ‰€åœ¨
- **add_mistake**: å°†é¢˜ç›®æ·»åŠ åˆ°é”™é¢˜æœ¬
- **generate_review_questions**: ç”Ÿæˆç»ƒä¹ é¢˜æ£€éªŒç†è§£

### ğŸ“Š è¿›åº¦è¿½è¸ª
- **get_today_tasks**: æŸ¥çœ‹ä»Šæ—¥ä»»åŠ¡
- **complete_task**: å®Œæˆå­¦ä¹ ä»»åŠ¡
- **get_learning_stats**: è·å–å­¦ä¹ ç»Ÿè®¡

## ç”¨æˆ·ç”»åƒ
{user_profile}

## é˜…è¯»ä¸Šä¸‹æ–‡
{reading_context}

## è¡Œä¸ºå‡†åˆ™
1. **æ·±å…¥æµ…å‡º**: ç”¨é€šä¿—æ˜“æ‡‚çš„è¯­è¨€è§£é‡Šå¤æ‚æ¦‚å¿µ
2. **ä¸¾ä¸€åä¸‰**: é€šè¿‡ä¾‹å­å’Œç±»æ¯”å¸®åŠ©ç†è§£
3. **äº’åŠ¨å­¦ä¹ **: é€‚æ—¶æé—®ï¼Œæ£€éªŒç”¨æˆ·ç†è§£ç¨‹åº¦
4. **çŸ¥è¯†å…³è”**: å°†æ–°çŸ¥è¯†ä¸å·²å­¦å†…å®¹å»ºç«‹è”ç³»
5. **é¼“åŠ±æ€è€ƒ**: å¼•å¯¼ç”¨æˆ·ä¸»åŠ¨æ€è€ƒè€Œéè¢«åŠ¨æ¥å—
6. **å–„ç”¨å·¥å…·**: é‡åˆ°é—®é¢˜æ—¶ä¸»åŠ¨è°ƒç”¨å·¥å…·è¾…åŠ©

## å¯ä¿¡åº¦ä¸è¾¹ç•Œ
- å…³é”®ç»“è®ºç»™ã€Œä¾æ®ã€ä¸ã€Œä¿¡å¿ƒã€æç¤ºï¼›ä¸ç¡®å®šå°±å…ˆé—®æ¾„æ¸…é—®é¢˜
- æ¶‰åŠåŒ»ç–—/æ³•å¾‹ç­‰æ•æ„Ÿå»ºè®®æ—¶ï¼Œç»™å‡ºè¾¹ç•Œæç¤ºå¹¶å»ºè®®å¯»æ±‚ä¸“ä¸šæ„è§

å½“å‰æ—¶é—´: {current_time}
"""


class LearningAgent:
    """
    AI å­¦ä¹ æ•™ç»ƒ/ä¼´è¯» Agent
    
    åŸºäº LangChain 1.0 + LangGraph å®ç°
    - ä½¿ç”¨ create_react_agent åˆ›å»º ReAct é£æ ¼çš„æ™ºèƒ½ä½“
    - æ”¯æŒå·¥å…·è°ƒç”¨å’Œå¤šè½®å¯¹è¯
    - å†…ç½®è®°å¿†ç®¡ç†å’Œç”¨æˆ·ç”»åƒ
    - æ™ºèƒ½æ¨¡å‹è·¯ç”±ï¼šæ ¹æ®ç”¨æˆ·é…ç½®å’Œæ¶ˆæ¯ç±»å‹åŠ¨æ€é€‰æ‹©æ¨¡å‹
    """
    
    def __init__(
        self,
        user_id: str,
        mode: str = "coach",  # "coach" æˆ– "reader"
        memory: Optional[AgentMemory] = None,
    ):
        self.user_id = user_id
        self.mode = mode
        self.memory = memory or AgentMemory(user_id)
        
        # LLM å®ä¾‹ç¼“å­˜ï¼ˆæŒ‰æ¨¡å‹ç±»å‹ï¼‰
        self._llm_cache: Dict[str, ChatOpenAI] = {}
        self._current_llm: Optional[ChatOpenAI] = None
        self._current_model_info: Optional[Dict[str, Any]] = None
        
        # è·å–å·¥å…·
        self.tools = get_all_tools(user_id=user_id, memory=self.memory)
        
        # LangGraph æ£€æŸ¥ç‚¹ï¼ˆç”¨äºå¯¹è¯çŠ¶æ€æŒä¹…åŒ–ï¼‰
        self.checkpointer = MemorySaver()
        
        # Agent å®ä¾‹ï¼ˆå»¶è¿Ÿåˆ›å»ºï¼Œç­‰å¾…æ¨¡å‹é…ç½®åŠ è½½ï¼‰
        self.agent = None
    
    def _create_llm(
        self,
        model_config: Dict[str, Any],
    ) -> Union[ChatOpenAI, "ChatGeminiCustom"]:
        """
        æ ¹æ®æ¨¡å‹é…ç½®åˆ›å»º LLM å®ä¾‹
        
        æ”¯æŒä¸¤ç§ LLM ç±»å‹ï¼š
        - ChatOpenAI: ç”¨äº OpenAI å…¼å®¹ APIï¼ˆDeepSeekã€OpenRouter ç­‰ï¼‰
        - ChatGeminiCustom: ç”¨äºåŸç”Ÿ Gemini API æ ¼å¼ï¼ˆæ”¯æŒè‡ªå®šä¹‰ base_urlï¼‰
        
        Args:
            model_config: æ¨¡å‹é…ç½®ï¼ŒåŒ…å« platform, model, base_url, api_key, api_format
            
        Returns:
            ChatOpenAI æˆ– ChatGeminiCustom å®ä¾‹
        """
        import httpx
        from .gemini_chat import ChatGeminiCustom
        
        platform = model_config.get("platform", "deepseek")
        model = model_config.get("model", "deepseek-chat")
        base_url = model_config.get("base_url", settings.DEEPSEEK_API_BASE)
        api_key = model_config.get("api_key", "")  # API Key å¿…é¡»ä»ç”¨æˆ·é…ç½®è·å–
        api_format = model_config.get("api_format", "openai")  # é»˜è®¤ OpenAI å…¼å®¹æ ¼å¼
        
        if not api_key:
            logger.warning(f"[LearningAgent] API Key æœªé…ç½®: platform={platform}, model={model}")
        
        logger.info(f"[LearningAgent] åˆ›å»º LLM: platform={platform}, model={model}, api_format={api_format}")
        
        # æ ¹æ® api_format é€‰æ‹© LLM ç±»å‹
        if api_format == "gemini":
            # ä½¿ç”¨è‡ªå®šä¹‰ Gemini LLMï¼ˆæ”¯æŒåŸç”Ÿ Gemini API æ ¼å¼å’ŒéŸ³é¢‘è¾“å…¥ï¼‰
            logger.info(f"[LearningAgent] ä½¿ç”¨ ChatGeminiCustom: base_url={base_url[:30]}...")
            return ChatGeminiCustom(
                model=model,
                api_key=api_key,
                base_url=base_url,
                temperature=0.7,
                streaming=True,
                timeout=120.0,
            )
        else:
            # ä½¿ç”¨ ChatOpenAIï¼ˆOpenAI å…¼å®¹æ ¼å¼ï¼‰
            # äº‘æ‰˜ç®¡ç¯å¢ƒä¸­å¯èƒ½å­˜åœ¨ SSL è¯ä¹¦é—®é¢˜ï¼Œé…ç½® HTTP å®¢æˆ·ç«¯
            http_client = None
            if IS_CLOUDRUN or DISABLE_SSL_VERIFY:
                http_client = httpx.Client(verify=False, http2=False, timeout=120.0)
            
            return ChatOpenAI(
                model=model,
                api_key=api_key,
                base_url=base_url,
                temperature=0.7,
                streaming=True,
                http_client=http_client,
            )
    
    async def _get_llm_for_message(
        self,
        multimodal: Optional[Dict[str, Any]] = None,
    ) -> Union[ChatOpenAI, Any]:
        """
        æ ¹æ®æ¶ˆæ¯ç±»å‹è·å–åˆé€‚çš„ LLM
        
        æ™ºèƒ½è·¯ç”±é€»è¾‘ï¼š
        - çº¯æ–‡æœ¬æ¶ˆæ¯ â†’ ç”¨æˆ·é…ç½®çš„æ–‡æœ¬æ¨¡å‹
        - å›¾ç‰‡æ¶ˆæ¯ â†’ ç”¨æˆ·é…ç½®çš„å¤šæ¨¡æ€/è§†è§‰æ¨¡å‹
        - è¯­éŸ³æ¶ˆæ¯ â†’ ç”¨æˆ·é…ç½®çš„è¯­éŸ³æ¨¡å‹ï¼ˆæˆ–é™çº§åˆ°æ–‡æœ¬æ¨¡å‹ï¼‰
        
        æ ¹æ® api_format è¿”å›ä¸åŒçš„ LLM ç±»å‹ï¼š
        - api_format="openai" â†’ ChatOpenAI
        - api_format="gemini" â†’ ChatGeminiCustom
        
        Args:
            multimodal: å¤šæ¨¡æ€æ¶ˆæ¯å­—å…¸
            
        Returns:
            ChatOpenAI æˆ– ChatGeminiCustom å®ä¾‹
        """
        # æ£€æµ‹æ¶ˆæ¯ç±»å‹
        model_type = "text"  # é»˜è®¤æ–‡æœ¬
        
        if multimodal:
            has_image = bool(multimodal.get("image_url") or multimodal.get("image_base64"))
            has_voice = bool(multimodal.get("voice_url"))
            
            if has_image:
                model_type = "multimodal"
            elif has_voice:
                model_type = "voice"
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"{self.user_id}:{model_type}"
        if cache_key in self._llm_cache:
            logger.debug(f"[LearningAgent] ä½¿ç”¨ç¼“å­˜çš„ LLM: type={model_type}")
            return self._llm_cache[cache_key]
        
        # ä» ModelConfigService è·å–ç”¨æˆ·é…ç½®çš„æ¨¡å‹
        try:
            model_config = await ModelConfigService.get_model_for_type(
                openid=self.user_id,
                model_type=model_type,
            )
            
            is_user_config = model_config.get("is_user_config", False)
            platform = model_config.get("platform", "unknown")
            model = model_config.get("model", "unknown")
            
            if is_user_config:
                logger.info(f"[LearningAgent] ä½¿ç”¨ç”¨æˆ·é…ç½®çš„æ¨¡å‹: type={model_type}, platform={platform}, model={model}")
            else:
                logger.info(f"[LearningAgent] ä½¿ç”¨ç³»ç»Ÿé»˜è®¤æ¨¡å‹: type={model_type}, platform={platform}, model={model}")
            
            # ä¿å­˜å½“å‰æ¨¡å‹ä¿¡æ¯ï¼ˆç”¨äºæ—¥å¿—å’Œè°ƒè¯•ï¼‰
            self._current_model_info = {
                "type": model_type,
                "platform": platform,
                "model": model,
                "is_user_config": is_user_config,
            }
            
        except Exception as e:
            logger.error(f"[LearningAgent] è·å–æ¨¡å‹é…ç½®å¤±è´¥: {e}")
            # é™çº§åˆ°ç³»ç»Ÿé»˜è®¤é…ç½®ï¼ˆæ—  API Keyï¼Œä¼šåœ¨è°ƒç”¨æ—¶å¤±è´¥ï¼‰
            model_config = {
                "platform": "deepseek",
                "model": settings.DEEPSEEK_MODEL,
                "base_url": settings.DEEPSEEK_API_BASE,
                "api_key": "",  # æ— æ³•è·å–ç”¨æˆ·é…ç½®æ—¶ï¼ŒAPI Key ä¸ºç©º
                "is_user_config": False,
            }
            self._current_model_info = {
                "type": model_type,
                "platform": "deepseek",
                "model": settings.DEEPSEEK_MODEL,
                "is_user_config": False,
                "fallback_reason": str(e),
            }
        
        # åˆ›å»º LLM å®ä¾‹
        llm = self._create_llm(model_config)
        
        # ç¼“å­˜ LLM å®ä¾‹
        self._llm_cache[cache_key] = llm
        self._current_llm = llm
        
        return llm
    
    def _create_agent(self, llm: Union[ChatOpenAI, Any]):
        """
        åˆ›å»º LangGraph ReAct Agent
        
        LangChain 1.0 æ¨èä½¿ç”¨ LangGraph çš„ create_react_agent
        è¿™æ˜¯ä¸€ä¸ªæ›´çµæ´»ã€å¯æ§çš„ Agent å®ç°æ–¹å¼
        
        æ³¨æ„ï¼šLangGraph 0.2.x+ ä¸­ state_modifier å‚æ•°å·²è¢«ç§»é™¤
        ç³»ç»Ÿæç¤ºç°åœ¨é€šè¿‡ SystemMessage åœ¨ chat() å’Œ chat_stream() ä¸­åŠ¨æ€æ·»åŠ 
        è¿™æ ·å¯ä»¥æ”¯æŒåŠ¨æ€çš„ç”¨æˆ·ç”»åƒå’Œå¯¹è¯æ‘˜è¦æ³¨å…¥
        
        Args:
            llm: ChatOpenAI æˆ– ChatGeminiCustom å®ä¾‹ï¼ˆæ ¹æ®æ¶ˆæ¯ç±»å‹åŠ¨æ€é€‰æ‹©ï¼‰
        """
        # ä½¿ç”¨ LangGraph åˆ›å»º ReAct Agent
        # create_react_agent è¿”å›ä¸€ä¸ª CompiledGraph
        # ç³»ç»Ÿæç¤ºé€šè¿‡ _build_system_message() åŠ¨æ€æ„å»ºå¹¶ä½œä¸º SystemMessage æ·»åŠ 
        self.agent = create_react_agent(
            model=llm,
            tools=self.tools,
            checkpointer=self.checkpointer,  # å¯ç”¨å¯¹è¯çŠ¶æ€æŒä¹…åŒ–
        )
    
    def _build_multimodal_content(
        self,
        multimodal: Dict[str, Any],
        is_multimodal_model: bool = False,
        voice_base64: Optional[str] = None,
        voice_format: str = "mp3",
    ) -> Union[str, List[Dict[str, Any]]]:
        """
        æ„å»ºå¤šæ¨¡æ€æ¶ˆæ¯å†…å®¹
        
        æ ¹æ®æ¨¡å‹èƒ½åŠ›é€‰æ‹©ä¸åŒçš„æ„å»ºæ–¹å¼ï¼š
        - å¤šæ¨¡æ€æ¨¡å‹ï¼ˆå¦‚ GPT-4oã€Geminiï¼‰ï¼šç›´æ¥æ„å»ºåŒ…å«å›¾ç‰‡/éŸ³é¢‘çš„æ¶ˆæ¯åˆ—è¡¨
        - çº¯æ–‡æœ¬æ¨¡å‹ï¼ˆå¦‚ DeepSeekï¼‰ï¼šè½¬æ¢ä¸ºæ–‡æœ¬æç¤ºï¼Œè®© Agent è°ƒç”¨å·¥å…·å¤„ç†
        
        Args:
            multimodal: å¤šæ¨¡æ€æ¶ˆæ¯å­—å…¸ï¼ŒåŒ…å« text, image_url, image_base64, voice_url, voice_text
            is_multimodal_model: å½“å‰æ¨¡å‹æ˜¯å¦æ”¯æŒå¤šæ¨¡æ€è¾“å…¥
            voice_base64: è¯­éŸ³çš„ base64 ç¼–ç ï¼ˆç”¨äºç›´æ¥å‘é€ç»™æ”¯æŒéŸ³é¢‘çš„æ¨¡å‹ï¼‰
            voice_format: è¯­éŸ³æ ¼å¼ï¼ˆmp3, wav ç­‰ï¼‰
            
        Returns:
            - å¤šæ¨¡æ€æ¨¡å‹ï¼šè¿”å›æ¶ˆæ¯å†…å®¹åˆ—è¡¨ [{"type": "text", ...}, {"type": "image_url", ...}, {"type": "input_audio", ...}]
            - çº¯æ–‡æœ¬æ¨¡å‹ï¼šè¿”å›çº¯æ–‡æœ¬å­—ç¬¦ä¸²
        """
        text = multimodal.get("text", "")
        image_url = multimodal.get("image_url")
        image_base64 = multimodal.get("image_base64")
        voice_text = multimodal.get("voice_text")
        
        # åˆå¹¶æ–‡æœ¬å†…å®¹
        text_parts = []
        if text:
            text_parts.append(text)
        if voice_text:
            if not text:
                text_parts.append(voice_text)
            else:
                text_parts.append(f"\n[è¯­éŸ³å†…å®¹]: {voice_text}")
        
        combined_text = "".join(text_parts).strip()
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å›¾ç‰‡æˆ–éŸ³é¢‘éœ€è¦å¤„ç†
        has_image = image_url or image_base64
        has_audio = voice_base64 is not None
        
        # å¦‚æœæ˜¯å¤šæ¨¡æ€æ¨¡å‹ä¸”æœ‰å›¾ç‰‡æˆ–éŸ³é¢‘ï¼Œç›´æ¥æ„å»ºå¤šæ¨¡æ€æ¶ˆæ¯
        if is_multimodal_model and (has_image or has_audio):
            content_parts = []
            
            # æ·»åŠ æ–‡æœ¬éƒ¨åˆ†
            if combined_text:
                content_parts.append({"type": "text", "text": combined_text})
            elif has_audio:
                # å¦‚æœåªæœ‰éŸ³é¢‘æ²¡æœ‰æ–‡æœ¬ï¼Œæ·»åŠ é»˜è®¤æç¤º
                # æ˜ç¡®è¦æ±‚ç”¨ä¸­æ–‡å›å¤ï¼Œå¹¶å‘Šè¯‰æ¨¡å‹è¿™æ˜¯è¯­éŸ³è¾“å…¥
                content_parts.append({
                    "type": "text", 
                    "text": "è¿™æ˜¯ç”¨æˆ·å‘é€çš„è¯­éŸ³æ¶ˆæ¯ï¼Œè¯·å…ˆå¬æ‡‚è¯­éŸ³å†…å®¹ï¼Œç„¶åç”¨ä¸­æ–‡å›å¤ç”¨æˆ·çš„é—®é¢˜æˆ–è¯·æ±‚ã€‚"
                })
            else:
                # å¦‚æœåªæœ‰å›¾ç‰‡æ²¡æœ‰æ–‡æœ¬ï¼Œæ·»åŠ é»˜è®¤æç¤º
                content_parts.append({"type": "text", "text": "è¯·ç”¨ä¸­æ–‡åˆ†æè¿™å¼ å›¾ç‰‡çš„å†…å®¹ã€‚"})
            
            # æ·»åŠ å›¾ç‰‡éƒ¨åˆ†
            if image_url:
                content_parts.append({
                    "type": "image_url",
                    "image_url": {"url": image_url}
                })
            elif image_base64:
                # Base64 æ ¼å¼å›¾ç‰‡
                content_parts.append({
                    "type": "image_url",
                    "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}
                })
            
            # æ·»åŠ éŸ³é¢‘éƒ¨åˆ†ï¼ˆç›´æ¥å‘é€ç»™æ”¯æŒéŸ³é¢‘è¾“å…¥çš„æ¨¡å‹ï¼‰
            if has_audio:
                content_parts.append({
                    "type": "input_audio",
                    "input_audio": {
                        "data": voice_base64,
                        "format": voice_format,
                    }
                })
                logger.info(f"[LearningAgent] æ·»åŠ éŸ³é¢‘å†…å®¹: format={voice_format}, size={len(voice_base64)} chars")
            
            logger.info(f"[LearningAgent] æ„å»ºå¤šæ¨¡æ€æ¶ˆæ¯: {len(content_parts)} éƒ¨åˆ†")
            return content_parts
        
        # çº¯æ–‡æœ¬æ¨¡å‹ï¼šè½¬æ¢ä¸ºæ–‡æœ¬æç¤º
        parts = []
        if combined_text:
            parts.append(combined_text)
        
        # å›¾ç‰‡éƒ¨åˆ† - è½¬æ¢ä¸ºæ–‡æœ¬æç¤ºï¼Œè®© Agent è°ƒç”¨ recognize_image å·¥å…·
        if image_url:
            parts.append(f"\n\n[ç”¨æˆ·ä¸Šä¼ äº†ä¸€å¼ å›¾ç‰‡ï¼Œè¯·ä½¿ç”¨ recognize_image å·¥å…·è¯†åˆ«å›¾ç‰‡å†…å®¹]\nå›¾ç‰‡URL: {image_url}")
        elif image_base64:
            parts.append("\n\n[ç”¨æˆ·ä¸Šä¼ äº†ä¸€å¼ å›¾ç‰‡ï¼ˆBase64æ ¼å¼ï¼‰ï¼Œä½†å½“å‰æ— æ³•ç›´æ¥å¤„ç†ã€‚è¯·å‘ŠçŸ¥ç”¨æˆ·é‡æ–°ä¸Šä¼ å›¾ç‰‡ã€‚]")
        
        result = "".join(parts).strip()
        return result if result else ""
    
    async def _transcribe_voice(self, voice_url: str) -> str:
        """
        è½¬å½•è¯­éŸ³ä¸ºæ–‡æœ¬
        
        ä½¿ç”¨ç”¨æˆ·é…ç½®çš„è¯­éŸ³æ¨¡å‹è¿›è¡Œè¯­éŸ³è½¬æ–‡æœ¬ï¼ˆSTTï¼‰
        æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
        1. ä¼ ç»Ÿ STT APIï¼ˆOpenAI Whisperã€é€šä¹‰åƒé—® Paraformerï¼‰
        2. å¤šæ¨¡æ€ Chat APIï¼ˆOpenRouterã€Gemini ç­‰ï¼Œé€šè¿‡ base64 éŸ³é¢‘ï¼‰
        
        Args:
            voice_url: è¯­éŸ³æ–‡ä»¶ URL
            
        Returns:
            è½¬å½•åçš„æ–‡æœ¬
        """
        import httpx
        import base64
        from ..config import get_http_client_kwargs
        
        logger.info(f"[LearningAgent] å¼€å§‹è½¬å½•è¯­éŸ³: {voice_url[:50]}...")
        
        try:
            # è·å–ç”¨æˆ·é…ç½®çš„è¯­éŸ³æ¨¡å‹
            voice_config = await ModelConfigService.get_model_for_type(
                openid=self.user_id,
                model_type="voice",
            )
            
            api_key = voice_config.get("api_key", "")
            platform = voice_config.get("platform", "")
            base_url = voice_config.get("base_url", "")
            model = voice_config.get("model", "")
            
            if not api_key:
                raise ValueError("è¯·å…ˆåœ¨ã€Œä¸ªäººä¸­å¿ƒ â†’ æ¨¡å‹é…ç½®ã€ä¸­é…ç½®è¯­éŸ³æ¨¡å‹çš„ API Key")
            
            logger.info(f"[LearningAgent] ä½¿ç”¨è¯­éŸ³æ¨¡å‹: platform={platform}, model={model}, base_url={base_url[:30]}...")
            
            async with httpx.AsyncClient(**get_http_client_kwargs(90.0)) as client:
                # ä¸‹è½½éŸ³é¢‘
                audio_response = await client.get(voice_url, follow_redirects=True)
                if audio_response.status_code != 200:
                    raise ValueError(f"ä¸‹è½½éŸ³é¢‘å¤±è´¥: HTTP {audio_response.status_code}")
                
                audio_data = audio_response.content
                content_type = audio_response.headers.get("content-type", "")
                
                # æ¨æ–­æ–‡ä»¶æ ¼å¼å’Œ MIME ç±»å‹
                if "mp3" in content_type or voice_url.endswith(".mp3"):
                    audio_format = "mp3"
                    mime_type = "audio/mpeg"
                elif "wav" in content_type or voice_url.endswith(".wav"):
                    audio_format = "wav"
                    mime_type = "audio/wav"
                elif "silk" in voice_url or "amr" in content_type:
                    # å¾®ä¿¡è¯­éŸ³æ ¼å¼ - å°è¯•ä½œä¸º mp3 å¤„ç†
                    audio_format = "mp3"
                    mime_type = "audio/mpeg"
                else:
                    audio_format = "mp3"
                    mime_type = "audio/mpeg"
                
                # åˆ¤æ–­ä½¿ç”¨å“ªç§ API æ¨¡å¼
                # qwen-omni æ¨¡å‹è™½ç„¶æ”¯æŒéŸ³é¢‘è¾“å…¥ï¼Œä½†ç”¨äºè½¬å½•æ—¶åº”ä½¿ç”¨ STT APIï¼ˆparaformerï¼‰
                # Chat API é€‚ç”¨äºéœ€è¦å¯¹è¯èƒ½åŠ›çš„åœºæ™¯ï¼Œçº¯è½¬å½•ç”¨ STT æ›´é«˜æ•ˆ
                model_lower = model.lower()
                is_qwen_omni = any(pattern in model_lower for pattern in ["qwen-omni", "qwen2.5-omni", "qwen3-omni", "qwen-audio"])
                is_qwen_platform = platform == "qwen" or platform.startswith("qwen")
                
                logger.info(f"[LearningAgent] è¯­éŸ³è½¬å½•è·¯ç”±åˆ¤æ–­: model={model}, platform={platform}, is_qwen_omni={is_qwen_omni}, is_qwen_platform={is_qwen_platform}")
                
                # è·å–ç”¨æˆ·é…ç½®çš„ API æ ¼å¼
                api_format = voice_config.get("api_format", "openai")
                
                # qwen å¹³å°ï¼ˆåŒ…æ‹¬ omni æ¨¡å‹ï¼‰ä½¿ç”¨ STT API
                # å…¶ä»–å¤šæ¨¡æ€æ¨¡å‹ä½¿ç”¨ Chat API
                use_chat_api = (
                    not is_qwen_omni and  # qwen-omni ç”¨ STT API
                    not is_qwen_platform and  # qwen å¹³å°ç”¨ STT API
                    (
                        platform.startswith("custom_") or
                        "openrouter" in base_url.lower() or
                        "gemini" in model_lower or
                        "claude" in model_lower or
                        "gpt-4" in model_lower
                    )
                )
                
                # æ£€æµ‹æ˜¯å¦ä½¿ç”¨åŸç”Ÿ Gemini API æ ¼å¼
                # ä¼˜å…ˆä½¿ç”¨ç”¨æˆ·é…ç½®çš„ api_formatï¼Œå…¶æ¬¡æ ¹æ® URL å’Œæ¨¡å‹åæ¨æ–­
                is_gemini_native = (
                    api_format == "gemini" or  # ç”¨æˆ·æ˜ç¡®é…ç½®ä¸º Gemini æ ¼å¼
                    (
                        "gemini" in model_lower and
                        (
                            "generativelanguage.googleapis.com" in base_url.lower() or
                            # è‡ªå®šä¹‰ä¸­è½¬ API é€šå¸¸ä¸åŒ…å« openrouterï¼Œä¸”æ¨¡å‹ååŒ…å« gemini
                            (platform.startswith("custom_") and "openrouter" not in base_url.lower() and api_format != "openai")
                        )
                    )
                )
                
                logger.info(f"[LearningAgent] è¯­éŸ³è½¬å½•è·¯ç”±ç»“æœ: use_chat_api={use_chat_api}, is_gemini_native={is_gemini_native}, api_format={api_format}")
                
                if is_gemini_native:
                    # ä½¿ç”¨åŸç”Ÿ Gemini API æ ¼å¼ï¼ˆinline_dataï¼‰
                    return await self._transcribe_via_gemini_native_api(
                        client, audio_data, audio_format, mime_type,
                        base_url, api_key, model
                    )
                elif use_chat_api:
                    # ä½¿ç”¨ Chat Completions APIï¼ˆOpenRouter ç­‰ OpenAI å…¼å®¹æ ¼å¼ï¼‰
                    return await self._transcribe_via_chat_api(
                        client, audio_data, audio_format, mime_type,
                        base_url, api_key, model
                    )
                else:
                    # ä½¿ç”¨ä¼ ç»Ÿ STT API
                    return await self._transcribe_via_stt_api(
                        client, audio_data, audio_format, mime_type,
                        base_url, api_key, platform
                    )
                
        except Exception as e:
            logger.error(f"[LearningAgent] è¯­éŸ³è½¬å½•å¤±è´¥: {e}")
            raise
    
    async def _download_and_encode_audio(self, voice_url: str) -> tuple:
        """
        ä¸‹è½½éŸ³é¢‘å¹¶ç¼–ç ä¸º base64
        
        Args:
            voice_url: è¯­éŸ³æ–‡ä»¶ URL
            
        Returns:
            (audio_base64, audio_format) å…ƒç»„
        """
        import httpx
        import base64
        from ..config import get_http_client_kwargs
        
        logger.info(f"[LearningAgent] ä¸‹è½½éŸ³é¢‘: {voice_url[:50]}...")
        
        async with httpx.AsyncClient(**get_http_client_kwargs(60.0)) as client:
            audio_response = await client.get(voice_url, follow_redirects=True)
            if audio_response.status_code != 200:
                raise ValueError(f"ä¸‹è½½éŸ³é¢‘å¤±è´¥: HTTP {audio_response.status_code}")
            
            audio_data = audio_response.content
            content_type = audio_response.headers.get("content-type", "")
            
            # æ¨æ–­æ–‡ä»¶æ ¼å¼
            if "mp3" in content_type or voice_url.endswith(".mp3"):
                audio_format = "mp3"
            elif "wav" in content_type or voice_url.endswith(".wav"):
                audio_format = "wav"
            elif "silk" in voice_url or "amr" in content_type:
                audio_format = "mp3"
            else:
                audio_format = "mp3"
            
            # Base64 ç¼–ç 
            audio_base64 = base64.b64encode(audio_data).decode("utf-8")
            
            logger.info(f"[LearningAgent] éŸ³é¢‘ä¸‹è½½å®Œæˆ: format={audio_format}, size={len(audio_data)} bytes")
            
            return audio_base64, audio_format
    
    async def _transcribe_via_gemini_native_api(
        self,
        client,
        audio_data: bytes,
        audio_format: str,
        mime_type: str,
        base_url: str,
        api_key: str,
        model: str,
    ) -> str:
        """
        é€šè¿‡åŸç”Ÿ Gemini API æ ¼å¼è½¬å½•è¯­éŸ³
        
        ä½¿ç”¨ /v1beta/models/{model}:generateContent ç«¯ç‚¹
        éŸ³é¢‘ä»¥ base64 ç¼–ç é€šè¿‡ inline_data ä¼ é€’
        
        è¯·æ±‚æ ¼å¼ï¼š
        {
            "contents": [{
                "role": "user",
                "parts": [
                    {"text": "è¯·è½¬å½•è¿™æ®µéŸ³é¢‘"},
                    {"inline_data": {"mime_type": "audio/mp3", "data": "base64..."}}
                ]
            }]
        }
        """
        import base64
        
        # Base64 ç¼–ç éŸ³é¢‘
        audio_base64 = base64.b64encode(audio_data).decode("utf-8")
        
        logger.info(f"[LearningAgent] ä½¿ç”¨åŸç”Ÿ Gemini API è½¬å½•ï¼ŒéŸ³é¢‘å¤§å°: {len(audio_data)} bytes, æ ¼å¼: {audio_format}")
        
        # æ„å»ºè¯·æ±‚ URL
        # åŸç”Ÿ Gemini API: /v1beta/models/{model}:generateContent?key={api_key}
        # ä¸­è½¬ API å¯èƒ½æ˜¯: {base_url}/v1beta/models/{model}:generateContent?key={api_key}
        base_url_clean = base_url.rstrip('/')
        
        # ç§»é™¤å¯èƒ½å­˜åœ¨çš„ /v1 åç¼€ï¼ˆç”¨æˆ·å¯èƒ½é…ç½®äº† OpenAI å…¼å®¹æ ¼å¼çš„ base_urlï¼‰
        if base_url_clean.endswith('/v1'):
            base_url_clean = base_url_clean[:-3]
        
        gemini_url = f"{base_url_clean}/v1beta/models/{model}:generateContent?key={api_key}"
        
        headers = {
            "Content-Type": "application/json",
        }
        
        # æ„å»ºåŸç”Ÿ Gemini æ ¼å¼çš„è¯·æ±‚ä½“
        payload = {
            "contents": [
                {
                    "role": "user",
                    "parts": [
                        {
                            "text": "è¯·å°†è¿™æ®µéŸ³é¢‘è½¬å½•ä¸ºæ–‡å­—ï¼Œåªè¾“å‡ºè½¬å½•çš„æ–‡å­—å†…å®¹ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–é¢å¤–å†…å®¹ã€‚"
                        },
                        {
                            "inline_data": {
                                "mime_type": mime_type,
                                "data": audio_base64
                            }
                        }
                    ]
                }
            ]
        }
        
        logger.info(f"[LearningAgent] Gemini Native API URL: {gemini_url[:80]}...")
        
        response = await client.post(
            gemini_url,
            headers=headers,
            json=payload,
            timeout=90.0,
        )
        
        if response.status_code != 200:
            error_text = response.text[:300] if response.text else "æœªçŸ¥é”™è¯¯"
            logger.error(f"[LearningAgent] Gemini Native API è½¬å½•å¤±è´¥: {response.status_code} - {error_text}")
            raise ValueError(f"è¯­éŸ³è½¬æ–‡æœ¬å¤±è´¥: {error_text}")
        
        result = response.json()
        
        # æå–è½¬å½•æ–‡æœ¬ï¼ˆåŸç”Ÿ Gemini å“åº”æ ¼å¼ï¼‰
        # å“åº”æ ¼å¼: {"candidates": [{"content": {"parts": [{"text": "..."}]}}]}
        candidates = result.get("candidates", [])
        if candidates:
            content = candidates[0].get("content", {})
            parts = content.get("parts", [])
            if parts:
                text = parts[0].get("text", "")
                logger.info(f"[LearningAgent] Gemini Native API è½¬å½•æˆåŠŸ: {text[:50]}...")
                return text.strip()
        
        raise ValueError("è¯­éŸ³è½¬æ–‡æœ¬å¤±è´¥: Gemini å“åº”ä¸­æ²¡æœ‰å†…å®¹")

    async def _transcribe_via_chat_api(
        self,
        client,
        audio_data: bytes,
        audio_format: str,
        mime_type: str,
        base_url: str,
        api_key: str,
        model: str,
    ) -> str:
        """
        é€šè¿‡ Chat Completions API è½¬å½•è¯­éŸ³ï¼ˆé€‚ç”¨äº OpenRouterã€Gemini ç­‰å¤šæ¨¡æ€æ¨¡å‹ï¼‰
        
        éŸ³é¢‘ä»¥ base64 ç¼–ç é€šè¿‡ input_audio å†…å®¹ç±»å‹å‘é€
        """
        import base64
        
        # Base64 ç¼–ç éŸ³é¢‘
        audio_base64 = base64.b64encode(audio_data).decode("utf-8")
        
        logger.info(f"[LearningAgent] ä½¿ç”¨ Chat API è½¬å½•ï¼ŒéŸ³é¢‘å¤§å°: {len(audio_data)} bytes, æ ¼å¼: {audio_format}")
        
        # æ„å»ºè¯·æ±‚
        chat_url = f"{base_url.rstrip('/')}/chat/completions"
        
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json",
        }
        
        # æ„å»ºå¤šæ¨¡æ€æ¶ˆæ¯ï¼ˆåŒ…å«éŸ³é¢‘ï¼‰
        payload = {
            "model": model,
            "messages": [
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": "è¯·å°†è¿™æ®µéŸ³é¢‘è½¬å½•ä¸ºæ–‡å­—ï¼Œåªè¾“å‡ºè½¬å½•çš„æ–‡å­—å†…å®¹ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–é¢å¤–å†…å®¹ã€‚"
                        },
                        {
                            "type": "input_audio",
                            "input_audio": {
                                "data": audio_base64,
                                "format": audio_format,
                            }
                        }
                    ]
                }
            ],
            "max_tokens": 2000,
        }
        
        response = await client.post(
            chat_url,
            headers=headers,
            json=payload,
            timeout=90.0,
        )
        
        if response.status_code != 200:
            error_text = response.text[:300] if response.text else "æœªçŸ¥é”™è¯¯"
            logger.error(f"[LearningAgent] Chat API è½¬å½•å¤±è´¥: {response.status_code} - {error_text}")
            raise ValueError(f"è¯­éŸ³è½¬æ–‡æœ¬å¤±è´¥: {error_text}")
        
        result = response.json()
        
        # æå–è½¬å½•æ–‡æœ¬
        choices = result.get("choices", [])
        if choices:
            message = choices[0].get("message", {})
            text = message.get("content", "")
            logger.info(f"[LearningAgent] Chat API è½¬å½•æˆåŠŸ: {text[:50]}...")
            return text.strip()
        
        raise ValueError("è¯­éŸ³è½¬æ–‡æœ¬å¤±è´¥: å“åº”ä¸­æ²¡æœ‰å†…å®¹")
    
    async def _transcribe_via_stt_api(
        self,
        client,
        audio_data: bytes,
        audio_format: str,
        mime_type: str,
        base_url: str,
        api_key: str,
        platform: str,
    ) -> str:
        """
        é€šè¿‡ä¼ ç»Ÿ STT API è½¬å½•è¯­éŸ³ï¼ˆé€‚ç”¨äº OpenAI Whisperã€é€šä¹‰åƒé—®ç­‰ï¼‰
        """
        logger.info(f"[LearningAgent] ä½¿ç”¨ STT API è½¬å½•ï¼Œå¹³å°: {platform}")
        
        # æ ¹æ®å¹³å°é€‰æ‹© STT API
        if platform == "qwen":
            transcription_url = f"{base_url}/audio/transcriptions"
            stt_model = "paraformer-v2"
        elif platform == "openai":
            transcription_url = f"{base_url}/audio/transcriptions"
            stt_model = "whisper-1"
        else:
            transcription_url = f"{base_url}/audio/transcriptions"
            stt_model = "whisper-1"
        
        headers = {
            "Authorization": f"Bearer {api_key}",
        }
        
        filename = f"audio.{audio_format}"
        files = {
            "file": (filename, audio_data, mime_type),
        }
        data = {
            "model": stt_model,
        }
        
        response = await client.post(
            transcription_url,
            headers=headers,
            files=files,
            data=data,
            timeout=60.0,
        )
        
        if response.status_code != 200:
            error_text = response.text[:200] if response.text else "æœªçŸ¥é”™è¯¯"
            raise ValueError(f"è¯­éŸ³è½¬æ–‡æœ¬å¤±è´¥: {error_text}")
        
        result = response.json()
        text = result.get("text", "")
        
        logger.info(f"[LearningAgent] STT API è½¬å½•æˆåŠŸ: {text[:50]}...")
        return text
    
    async def chat(
        self,
        message: str = None,
        multimodal: Optional[Dict[str, Any]] = None,
        context: Optional[Dict[str, Any]] = None,
    ) -> str:
        """
        ä¸ Agent å¯¹è¯ï¼ˆéæµå¼ï¼‰- æ”¯æŒå¤šæ¨¡æ€
        
        Args:
            message: çº¯æ–‡æœ¬æ¶ˆæ¯ï¼ˆå‘åå…¼å®¹ï¼‰
            multimodal: å¤šæ¨¡æ€æ¶ˆæ¯ {text, image_url, image_base64, voice_url, voice_text}
            context: é¢å¤–ä¸Šä¸‹æ–‡ï¼ˆå¦‚å½“å‰é˜…è¯»çš„å†…å®¹ï¼‰
            
        Returns:
            Agent å›å¤
        """
        # æ™ºèƒ½æ¨¡å‹è·¯ç”±ï¼šæ ¹æ®æ¶ˆæ¯ç±»å‹è·å–åˆé€‚çš„ LLMï¼ˆéœ€è¦å…ˆè·å–ï¼Œæ‰èƒ½çŸ¥é“æ˜¯å¦æ”¯æŒå¤šæ¨¡æ€ï¼‰
        llm = await self._get_llm_for_message(multimodal)
        
        # åˆ¤æ–­å½“å‰æ¨¡å‹æ˜¯å¦æ”¯æŒå¤šæ¨¡æ€ï¼ˆç”¨æˆ·é…ç½®äº†å¤šæ¨¡æ€æ¨¡å‹ï¼‰
        is_multimodal_model = (
            self._current_model_info and 
            self._current_model_info.get("is_user_config", False)
        )
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å›¾ç‰‡
        has_image = multimodal and (multimodal.get("image_url") or multimodal.get("image_base64"))
        
        logger.info(f"[LearningAgent] å¤šæ¨¡æ€åˆ¤æ–­: is_multimodal_model={is_multimodal_model}, has_image={has_image}, model_info={self._current_model_info}")
        
        # ç”¨äºç›´æ¥å‘é€ç»™æ¨¡å‹çš„éŸ³é¢‘æ•°æ®
        voice_base64 = None
        voice_format = "mp3"
        
        # æ ‡è®°æ˜¯å¦è¿›è¡Œäº†è¯­éŸ³è½¬å½•ï¼ˆè½¬å½•åéœ€è¦é‡æ–°é€‰æ‹©æ–‡æœ¬æ¨¡å‹ï¼‰
        did_voice_transcription = False
        
        # æ£€æŸ¥å½“å‰ LLM æ˜¯å¦æ˜¯ ChatGeminiCustomï¼ˆå¯ä»¥ç›´æ¥å¤„ç†éŸ³é¢‘ï¼‰
        from .gemini_chat import ChatGeminiCustom
        is_gemini_llm = isinstance(llm, ChatGeminiCustom)
        
        # æ„å»ºæ¶ˆæ¯å†…å®¹
        if multimodal:
            # å¦‚æœæœ‰è¯­éŸ³ URLï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦è½¬å½•
            if multimodal.get("voice_url") and not multimodal.get("voice_text"):
                # è·å–è¯­éŸ³æ¨¡å‹é…ç½®ï¼Œæ£€æŸ¥æ˜¯å¦æ”¯æŒç›´æ¥éŸ³é¢‘è¾“å…¥
                voice_config = await ModelConfigService.get_model_for_type(
                    openid=self.user_id,
                    model_type="voice",
                )
                model_types = voice_config.get("model_types", [])
                api_format = voice_config.get("api_format", "openai")
                
                # æ£€æŸ¥æ¨¡å‹æ˜¯å¦åŒæ—¶æ”¯æŒ text å’Œ voiceï¼ˆå¯ä»¥ç›´æ¥å¤„ç†éŸ³é¢‘ï¼‰
                supports_direct_audio = "text" in model_types and "voice" in model_types
                
                logger.info(f"[LearningAgent] è¯­éŸ³æ¨¡å‹èƒ½åŠ›æ£€æŸ¥: model_types={model_types}, supports_direct_audio={supports_direct_audio}, api_format={api_format}, is_gemini_llm={is_gemini_llm}")
                
                # ChatGeminiCustom æ”¯æŒç›´æ¥å¤„ç†éŸ³é¢‘ï¼Œä¸éœ€è¦è½¬å½•
                if is_gemini_llm and supports_direct_audio:
                    # ä¸‹è½½éŸ³é¢‘å¹¶ç¼–ç ä¸º base64ï¼Œç›´æ¥å‘é€ç»™ Gemini
                    logger.info("[LearningAgent] ä½¿ç”¨ ChatGeminiCustom ç›´æ¥å¤„ç†éŸ³é¢‘")
                    voice_base64, voice_format = await self._download_and_encode_audio(multimodal["voice_url"])
                else:
                    # ChatOpenAI ä¸æ”¯æŒ input_audio å†…å®¹ç±»å‹ï¼Œéœ€è¦å…ˆè½¬å½•
                    # å³ä½¿æ¨¡å‹æ”¯æŒç›´æ¥éŸ³é¢‘è¾“å…¥ï¼ˆå¦‚ qwen-omniï¼‰ï¼Œä¹Ÿéœ€è¦å…ˆè½¬å½•ä¸ºæ–‡æœ¬
                    logger.info("[LearningAgent] ä½¿ç”¨è½¬å½•æ¨¡å¼å¤„ç†è¯­éŸ³")
                    try:
                        transcribed = await self._transcribe_voice(multimodal["voice_url"])
                        multimodal["voice_text"] = transcribed
                        did_voice_transcription = True
                    except Exception as e:
                        logger.error(f"[LearningAgent] è¯­éŸ³è½¬å½•å¤±è´¥ï¼Œé™çº§åˆ°æ–‡æœ¬: {e}")
            
            content = self._build_multimodal_content(
                multimodal, 
                is_multimodal_model or is_gemini_llm,  # Gemini ä¹Ÿæ”¯æŒå¤šæ¨¡æ€
                voice_base64=voice_base64,
                voice_format=voice_format,
            )
            # ç”¨äºè®°å½•çš„æ–‡æœ¬æ¶ˆæ¯
            text_for_log = multimodal.get("text") or multimodal.get("voice_text") or "[å¤šæ¨¡æ€æ¶ˆæ¯]"
        else:
            content = message
            text_for_log = message
        
        # è¯­éŸ³è½¬å½•åï¼Œå¦‚æœä½¿ç”¨çš„æ˜¯ ChatOpenAIï¼Œéœ€è¦é‡æ–°è·å–æ–‡æœ¬æ¨¡å‹è¿›è¡Œå¯¹è¯
        # å› ä¸ºè¯­éŸ³æ¨¡å‹å¯èƒ½ä½¿ç”¨é OpenAI å…¼å®¹çš„ API æ ¼å¼
        # ä½†å¦‚æœæ˜¯ ChatGeminiCustomï¼Œåˆ™ä¸éœ€è¦åˆ‡æ¢ï¼ˆå®ƒæœ¬èº«å°±æ”¯æŒå¯¹è¯ï¼‰
        if did_voice_transcription and not is_gemini_llm:
            logger.info("[LearningAgent] è¯­éŸ³è½¬å½•å®Œæˆï¼Œé‡æ–°è·å–æ–‡æœ¬æ¨¡å‹è¿›è¡Œå¯¹è¯")
            # æ¸…é™¤ç¼“å­˜çš„è¯­éŸ³æ¨¡å‹ LLMï¼Œå¼ºåˆ¶é‡æ–°è·å–æ–‡æœ¬æ¨¡å‹
            cache_key = f"{self.user_id}:voice"
            if cache_key in self._llm_cache:
                del self._llm_cache[cache_key]
            # è·å–æ–‡æœ¬æ¨¡å‹ï¼ˆä¼ å…¥ None è¡¨ç¤ºçº¯æ–‡æœ¬æ¶ˆæ¯ï¼‰
            llm = await self._get_llm_for_message(None)
            logger.info(f"[LearningAgent] åˆ‡æ¢åˆ°æ–‡æœ¬æ¨¡å‹: {self._current_model_info}")
        
        # åˆ›å»º/æ›´æ–° Agentï¼ˆä½¿ç”¨é€‰å®šçš„ LLMï¼‰
        self._create_agent(llm)
        
        # å‡†å¤‡è¾“å…¥
        input_data = self._prepare_input(text_for_log, context)
        
        # é…ç½®çº¿ç¨‹ IDï¼ˆç”¨äºå¤šè½®å¯¹è¯ï¼‰
        config = {"configurable": {"thread_id": self.user_id}}
        
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = [HumanMessage(content=content)]
        
        # å¦‚æœæœ‰ç³»ç»Ÿæç¤ºï¼Œæ·»åŠ ä¸Šä¸‹æ–‡
        if input_data.get("user_profile"):
            system_content = self._build_system_message(input_data)
            messages.insert(0, SystemMessage(content=system_content))
        
        # æ‰§è¡Œ Agent
        result = await self.agent.ainvoke(
            {"messages": messages},
            config=config,
        )
        
        # æå–æœ€ç»ˆå›å¤
        output = ""
        if result.get("messages"):
            last_message = result["messages"][-1]
            if hasattr(last_message, 'content'):
                output = last_message.content
        
        # ä¿å­˜å¯¹è¯è®°å½•
        await self.memory.add_message("user", text_for_log)
        await self.memory.add_message("assistant", output)
        
        # åˆ†æå¹¶æ›´æ–°ç”¨æˆ·ç”»åƒ
        await self._analyze_and_evolve(text_for_log, {"output": output})
        
        return output
    
    async def chat_stream(
        self,
        message: str = None,
        multimodal: Optional[Dict[str, Any]] = None,
        context: Optional[Dict[str, Any]] = None,
    ) -> AsyncIterator[Dict[str, Any]]:
        """
        ä¸ Agent å¯¹è¯ï¼ˆæµå¼ï¼‰- æ”¯æŒå¤šæ¨¡æ€
        
        ä½¿ç”¨ LangGraph çš„ astream_events API å®ç°æµå¼è¾“å‡º
        è¿”å›ç»“æ„åŒ–çš„äº‹ä»¶å¯¹è±¡ï¼Œä¾¿äºå‰ç«¯è§£æå±•ç¤º
        
        Args:
            message: çº¯æ–‡æœ¬æ¶ˆæ¯ï¼ˆå‘åå…¼å®¹ï¼‰
            multimodal: å¤šæ¨¡æ€æ¶ˆæ¯ {text, image_url, image_base64, voice_url, voice_text}
            context: é¢å¤–ä¸Šä¸‹æ–‡
            
        Yields:
            ç»“æ„åŒ–çš„äº‹ä»¶å¯¹è±¡:
            - type: "text" | "tool_start" | "tool_end" | "tool_error" | "thinking" | "transcription" | "model_info"
            - content: æ–‡æœ¬å†…å®¹
            - tool_name: å·¥å…·åç§°ï¼ˆå·¥å…·äº‹ä»¶æ—¶ï¼‰
            - tool_input: å·¥å…·è¾“å…¥å‚æ•°ï¼ˆtool_start æ—¶ï¼‰
            - tool_output: å·¥å…·è¾“å‡ºç»“æœï¼ˆtool_end æ—¶ï¼‰
            - text: è¯­éŸ³è½¬å½•æ–‡æœ¬ï¼ˆtranscription äº‹ä»¶æ—¶ï¼‰
            - model_info: å½“å‰ä½¿ç”¨çš„æ¨¡å‹ä¿¡æ¯ï¼ˆmodel_info äº‹ä»¶æ—¶ï¼‰
        """
        # æ™ºèƒ½æ¨¡å‹è·¯ç”±ï¼šæ ¹æ®æ¶ˆæ¯ç±»å‹è·å–åˆé€‚çš„ LLMï¼ˆéœ€è¦å…ˆè·å–ï¼Œæ‰èƒ½çŸ¥é“æ˜¯å¦æ”¯æŒå¤šæ¨¡æ€ï¼‰
        llm = await self._get_llm_for_message(multimodal)
        
        # åˆ¤æ–­å½“å‰æ¨¡å‹æ˜¯å¦æ”¯æŒå¤šæ¨¡æ€ï¼ˆç”¨æˆ·é…ç½®äº†å¤šæ¨¡æ€æ¨¡å‹ï¼‰
        is_multimodal_model = (
            self._current_model_info and 
            self._current_model_info.get("is_user_config", False)
        )
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å›¾ç‰‡
        has_image = multimodal and (multimodal.get("image_url") or multimodal.get("image_base64"))
        
        logger.info(f"[LearningAgent] æµå¼å¤šæ¨¡æ€åˆ¤æ–­: is_multimodal_model={is_multimodal_model}, has_image={has_image}, model_info={self._current_model_info}")
        
        # ç”¨äºç›´æ¥å‘é€ç»™æ¨¡å‹çš„éŸ³é¢‘æ•°æ®
        voice_base64 = None
        voice_format = "mp3"
        
        # æ ‡è®°æ˜¯å¦è¿›è¡Œäº†è¯­éŸ³è½¬å½•ï¼ˆè½¬å½•åéœ€è¦é‡æ–°é€‰æ‹©æ–‡æœ¬æ¨¡å‹ï¼‰
        did_voice_transcription = False
        
        # æ£€æŸ¥å½“å‰ LLM æ˜¯å¦æ˜¯ ChatGeminiCustomï¼ˆå¯ä»¥ç›´æ¥å¤„ç†éŸ³é¢‘ï¼‰
        from .gemini_chat import ChatGeminiCustom
        is_gemini_llm = isinstance(llm, ChatGeminiCustom)
        
        # æ„å»ºæ¶ˆæ¯å†…å®¹
        if multimodal:
            # å¦‚æœæœ‰è¯­éŸ³ URLï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦è½¬å½•
            if multimodal.get("voice_url") and not multimodal.get("voice_text"):
                # è·å–è¯­éŸ³æ¨¡å‹é…ç½®ï¼Œæ£€æŸ¥æ˜¯å¦æ”¯æŒç›´æ¥éŸ³é¢‘è¾“å…¥
                voice_config = await ModelConfigService.get_model_for_type(
                    openid=self.user_id,
                    model_type="voice",
                )
                model_types = voice_config.get("model_types", [])
                api_format = voice_config.get("api_format", "openai")
                
                # æ£€æŸ¥æ¨¡å‹æ˜¯å¦åŒæ—¶æ”¯æŒ text å’Œ voiceï¼ˆå¯ä»¥ç›´æ¥å¤„ç†éŸ³é¢‘ï¼‰
                supports_direct_audio = "text" in model_types and "voice" in model_types
                
                logger.info(f"[LearningAgent] è¯­éŸ³æ¨¡å‹èƒ½åŠ›æ£€æŸ¥: model_types={model_types}, supports_direct_audio={supports_direct_audio}, api_format={api_format}, is_gemini_llm={is_gemini_llm}")
                
                # ChatGeminiCustom æ”¯æŒç›´æ¥å¤„ç†éŸ³é¢‘ï¼Œä¸éœ€è¦è½¬å½•
                if is_gemini_llm and supports_direct_audio:
                    # ä¸‹è½½éŸ³é¢‘å¹¶ç¼–ç ä¸º base64ï¼Œç›´æ¥å‘é€ç»™ Gemini
                    logger.info("[LearningAgent] ä½¿ç”¨ ChatGeminiCustom ç›´æ¥å¤„ç†éŸ³é¢‘")
                    voice_base64, voice_format = await self._download_and_encode_audio(multimodal["voice_url"])
                    # å‘é€æç¤ºäº‹ä»¶ï¼ˆå‘ŠçŸ¥ç”¨æˆ·æ­£åœ¨å¤„ç†éŸ³é¢‘ï¼‰
                    yield {"type": "thinking", "content": "æ­£åœ¨å¤„ç†è¯­éŸ³..."}
                else:
                    # ChatOpenAI ä¸æ”¯æŒ input_audio å†…å®¹ç±»å‹ï¼Œéœ€è¦å…ˆè½¬å½•
                    logger.info("[LearningAgent] ä½¿ç”¨è½¬å½•æ¨¡å¼å¤„ç†è¯­éŸ³")
                    try:
                        transcribed = await self._transcribe_voice(multimodal["voice_url"])
                        multimodal["voice_text"] = transcribed
                        did_voice_transcription = True
                        # å‘é€è½¬å½•äº‹ä»¶
                        yield {"type": "transcription", "text": transcribed}
                    except Exception as e:
                        logger.error(f"[LearningAgent] è¯­éŸ³è½¬å½•å¤±è´¥: {e}")
                        yield {"type": "error", "error": f"è¯­éŸ³è½¬å½•å¤±è´¥: {str(e)}"}
                        return
            
            content = self._build_multimodal_content(
                multimodal, 
                is_multimodal_model or is_gemini_llm,  # Gemini ä¹Ÿæ”¯æŒå¤šæ¨¡æ€
                voice_base64=voice_base64,
                voice_format=voice_format,
            )
            # ç”¨äºè®°å½•çš„æ–‡æœ¬æ¶ˆæ¯
            text_for_log = multimodal.get("text") or multimodal.get("voice_text") or "[å¤šæ¨¡æ€æ¶ˆæ¯]"
        else:
            content = message
            text_for_log = message
        
        # è¯­éŸ³è½¬å½•åï¼Œå¦‚æœä½¿ç”¨çš„æ˜¯ ChatOpenAIï¼Œéœ€è¦é‡æ–°è·å–æ–‡æœ¬æ¨¡å‹è¿›è¡Œå¯¹è¯
        # å› ä¸ºè¯­éŸ³æ¨¡å‹å¯èƒ½ä½¿ç”¨é OpenAI å…¼å®¹çš„ API æ ¼å¼
        # ä½†å¦‚æœæ˜¯ ChatGeminiCustomï¼Œåˆ™ä¸éœ€è¦åˆ‡æ¢ï¼ˆå®ƒæœ¬èº«å°±æ”¯æŒå¯¹è¯ï¼‰
        if did_voice_transcription and not is_gemini_llm:
            logger.info("[LearningAgent] è¯­éŸ³è½¬å½•å®Œæˆï¼Œé‡æ–°è·å–æ–‡æœ¬æ¨¡å‹è¿›è¡Œå¯¹è¯")
            # æ¸…é™¤ç¼“å­˜çš„è¯­éŸ³æ¨¡å‹ LLMï¼Œå¼ºåˆ¶é‡æ–°è·å–æ–‡æœ¬æ¨¡å‹
            cache_key = f"{self.user_id}:voice"
            if cache_key in self._llm_cache:
                del self._llm_cache[cache_key]
            # è·å–æ–‡æœ¬æ¨¡å‹ï¼ˆä¼ å…¥ None è¡¨ç¤ºçº¯æ–‡æœ¬æ¶ˆæ¯ï¼‰
            llm = await self._get_llm_for_message(None)
            logger.info(f"[LearningAgent] åˆ‡æ¢åˆ°æ–‡æœ¬æ¨¡å‹: {self._current_model_info}")
        
        # åˆ›å»º/æ›´æ–° Agentï¼ˆä½¿ç”¨é€‰å®šçš„ LLMï¼‰
        self._create_agent(llm)
        
        # å‘é€æ¨¡å‹ä¿¡æ¯äº‹ä»¶ï¼ˆè®©å‰ç«¯çŸ¥é“ä½¿ç”¨äº†å“ªä¸ªæ¨¡å‹ï¼‰
        if self._current_model_info:
            yield {
                "type": "model_info",
                "model_info": self._current_model_info,
            }
        
        # å‡†å¤‡è¾“å…¥
        input_data = self._prepare_input(text_for_log, context)
        
        # é…ç½®
        config = {"configurable": {"thread_id": self.user_id}}
        
        # æ„å»ºæ¶ˆæ¯
        messages = [HumanMessage(content=content)]
        if input_data.get("user_profile"):
            system_content = self._build_system_message(input_data)
            messages.insert(0, SystemMessage(content=system_content))
        
        full_response = ""
        current_tool_calls = {}  # è¿½è¸ªå½“å‰å·¥å…·è°ƒç”¨
        
        # ä½¿ç”¨ astream_events è¿›è¡Œæµå¼å¤„ç†
        async for event in self.agent.astream_events(
            {"messages": messages},
            config=config,
            version="v2",
        ):
            kind = event["event"]
            
            # å¤„ç† LLM æµå¼è¾“å‡º
            if kind == "on_chat_model_stream":
                chunk = event.get("data", {}).get("chunk")
                if chunk and hasattr(chunk, 'content') and chunk.content:
                    content_chunk = chunk.content
                    full_response += content_chunk
                    yield {"type": "text", "content": content_chunk}
            
            # å¤„ç†å·¥å…·è°ƒç”¨å¼€å§‹
            elif kind == "on_tool_start":
                tool_name = event.get("name", "unknown")
                tool_input = event.get("data", {}).get("input", {})
                run_id = event.get("run_id", "")
                
                # è·å–å·¥å…·çš„ä¸­æ–‡åç§°å’Œæè¿°
                tool_info = self._get_tool_display_info(tool_name)
                
                current_tool_calls[run_id] = {
                    "name": tool_name,
                    "display_name": tool_info["display_name"],
                    "description": tool_info["description"],
                    "icon": tool_info["icon"],
                    "input": tool_input,
                }
                
                yield {
                    "type": "tool_start",
                    "tool_name": tool_name,
                    "display_name": tool_info["display_name"],
                    "description": tool_info["description"],
                    "icon": tool_info["icon"],
                    "tool_input": tool_input,
                    "run_id": run_id,
                }
            
            # å¤„ç†å·¥å…·è°ƒç”¨ç»“æŸ
            elif kind == "on_tool_end":
                run_id = event.get("run_id", "")
                tool_output = event.get("data", {}).get("output", "")
                
                tool_call_info = current_tool_calls.get(run_id, {})
                tool_name = tool_call_info.get("name", "unknown")
                
                # è§£æå·¥å…·è¾“å‡º
                parsed_output = self._parse_tool_output(tool_output)
                
                yield {
                    "type": "tool_end",
                    "tool_name": tool_name,
                    "display_name": tool_call_info.get("display_name", tool_name),
                    "icon": tool_call_info.get("icon", "ğŸ”§"),
                    "tool_output": parsed_output,
                    "success": parsed_output.get("success", True),
                    "run_id": run_id,
                }
                
                # æ¸…ç†å·²å®Œæˆçš„å·¥å…·è°ƒç”¨
                if run_id in current_tool_calls:
                    del current_tool_calls[run_id]
            
            # å¤„ç†å·¥å…·è°ƒç”¨é”™è¯¯
            elif kind == "on_tool_error":
                run_id = event.get("run_id", "")
                error = event.get("data", {}).get("error", "æœªçŸ¥é”™è¯¯")
                
                tool_call_info = current_tool_calls.get(run_id, {})
                
                yield {
                    "type": "tool_error",
                    "tool_name": tool_call_info.get("name", "unknown"),
                    "display_name": tool_call_info.get("display_name", "å·¥å…·"),
                    "icon": tool_call_info.get("icon", "ğŸ”§"),
                    "error": str(error),
                    "run_id": run_id,
                }
        
        # ä¿å­˜å¯¹è¯è®°å½•
        await self.memory.add_message("user", text_for_log)
        await self.memory.add_message("assistant", full_response)
        
        # å¼‚æ­¥åˆ†æå¹¶è¿›åŒ–
        await self._analyze_and_evolve(text_for_log, {"output": full_response})
    
    def _get_tool_display_info(self, tool_name: str) -> Dict[str, str]:
        """è·å–å·¥å…·çš„æ˜¾ç¤ºä¿¡æ¯ï¼ˆä¸­æ–‡åç§°ã€æè¿°ã€å›¾æ ‡ï¼‰"""
        tool_info_map = {
            # å­¦ä¹ è®¡åˆ’ç›¸å…³
            "create_learning_plan": {"display_name": "åˆ›å»ºå­¦ä¹ è®¡åˆ’", "description": "ä¸ºä½ åˆ¶å®šä¸ªæ€§åŒ–å­¦ä¹ è®¡åˆ’", "icon": "ğŸ“‹"},
            "generate_daily_tasks": {"display_name": "ç”Ÿæˆä»Šæ—¥ä»»åŠ¡", "description": "ç”Ÿæˆæ¯æ—¥å­¦ä¹ ä»»åŠ¡", "icon": "ğŸ“"},
            
            # æœç´¢ç›¸å…³
            "search_resources": {"display_name": "è”ç½‘æœç´¢", "description": "åœ¨ç½‘ä¸Šæœç´¢ç›¸å…³èµ„æ–™", "icon": "ğŸ”"},
            "search_learning_materials": {"display_name": "æœç´¢å­¦ä¹ èµ„æ–™", "description": "æœç´¢å­¦ä¹ ç›¸å…³ææ–™", "icon": "ğŸ“š"},
            
            # ä»»åŠ¡ç®¡ç†
            "get_today_tasks": {"display_name": "è·å–ä»Šæ—¥ä»»åŠ¡", "description": "æŸ¥çœ‹ä»Šå¤©çš„å­¦ä¹ ä»»åŠ¡", "icon": "ğŸ“‹"},
            "complete_task": {"display_name": "å®Œæˆä»»åŠ¡", "description": "æ ‡è®°ä»»åŠ¡ä¸ºå·²å®Œæˆ", "icon": "âœ…"},
            "get_task_progress": {"display_name": "ä»»åŠ¡è¿›åº¦", "description": "æŸ¥çœ‹ä»»åŠ¡å®Œæˆè¿›åº¦", "icon": "ğŸ“Š"},
            "suggest_task_adjustment": {"display_name": "è°ƒæ•´å»ºè®®", "description": "å»ºè®®è°ƒæ•´ä»»åŠ¡å®‰æ’", "icon": "ğŸ’¡"},
            
            # æ‰“å¡ç³»ç»Ÿ
            "do_checkin": {"display_name": "å­¦ä¹ æ‰“å¡", "description": "æ‰§è¡Œå­¦ä¹ æ‰“å¡ç­¾åˆ°", "icon": "âœ¨"},
            "get_checkin_status": {"display_name": "æ‰“å¡çŠ¶æ€", "description": "æŸ¥çœ‹æ‰“å¡ç»Ÿè®¡", "icon": "ğŸ“ˆ"},
            "get_badges": {"display_name": "æˆå°±å¾½ç« ", "description": "æŸ¥çœ‹è·å¾—çš„å¾½ç« ", "icon": "ğŸ…"},
            
            # ç•ªèŒ„ä¸“æ³¨
            "get_focus_stats": {"display_name": "ä¸“æ³¨ç»Ÿè®¡", "description": "æŸ¥çœ‹ä¸“æ³¨æ—¶é—´ç»Ÿè®¡", "icon": "ğŸ…"},
            "suggest_focus_plan": {"display_name": "ä¸“æ³¨è®¡åˆ’", "description": "å»ºè®®ä¸“æ³¨è®¡åˆ’å®‰æ’", "icon": "â±ï¸"},
            
            # é”™é¢˜æœ¬
            "get_mistakes": {"display_name": "é”™é¢˜åˆ—è¡¨", "description": "æŸ¥çœ‹é”™é¢˜æœ¬", "icon": "ğŸ“•"},
            "add_mistake": {"display_name": "æ·»åŠ é”™é¢˜", "description": "æ·»åŠ æ–°é”™é¢˜", "icon": "â•"},
            "analyze_mistake": {"display_name": "é”™é¢˜åˆ†æ", "description": "AIåˆ†æé”™é¢˜åŸå› ", "icon": "ğŸ”¬"},
            "generate_review_questions": {"display_name": "ç”Ÿæˆç»ƒä¹ é¢˜", "description": "ç”Ÿæˆå¤ä¹ é¢˜ç›®", "icon": "ğŸ“"},
            "mark_mistake_mastered": {"display_name": "æ ‡è®°å·²æŒæ¡", "description": "æ ‡è®°é”™é¢˜ä¸ºå·²æŒæ¡", "icon": "ğŸ¯"},
            
            # ç»Ÿè®¡åˆ†æ
            "get_learning_stats": {"display_name": "å­¦ä¹ ç»Ÿè®¡", "description": "è·å–å­¦ä¹ æ•°æ®ç»Ÿè®¡", "icon": "ğŸ“Š"},
            "get_ranking": {"display_name": "æ’è¡Œæ¦œ", "description": "æŸ¥çœ‹å­¦ä¹ æ’è¡Œæ¦œ", "icon": "ğŸ†"},
            "get_achievement_rate": {"display_name": "è¾¾æˆç‡", "description": "æŸ¥çœ‹ç›®æ ‡è¾¾æˆç‡", "icon": "ğŸ¯"},
            "analyze_learning_pattern": {"display_name": "å­¦ä¹ åˆ†æ", "description": "åˆ†æå­¦ä¹ æ¨¡å¼", "icon": "ğŸ“ˆ"},
            "get_calendar_data": {"display_name": "æ—¥å†æ•°æ®", "description": "æŸ¥çœ‹æ—¥å†å­¦ä¹ è¯¦æƒ…", "icon": "ğŸ“…"},
            "analyze_learning_status": {"display_name": "çŠ¶æ€åˆ†æ", "description": "åˆ†ææ•´ä½“å­¦ä¹ çŠ¶æ€", "icon": "ğŸ’¡"},
            
            # ç”¨æˆ·ç”»åƒ
            "update_user_profile": {"display_name": "æ›´æ–°ç”»åƒ", "description": "æ›´æ–°ç”¨æˆ·å­¦ä¹ ç”»åƒ", "icon": "ğŸ‘¤"},
            "get_user_stats": {"display_name": "ç”¨æˆ·ç»Ÿè®¡", "description": "è·å–ç”¨æˆ·ç»Ÿè®¡ä¿¡æ¯", "icon": "ğŸ“‹"},
            
            # æ–‡æ¡£ä¼´è¯»
            "get_documents": {"display_name": "æ–‡æ¡£åˆ—è¡¨", "description": "è·å–å­¦ä¹ æ–‡æ¡£åˆ—è¡¨", "icon": "ğŸ“š"},
            "search_documents": {"display_name": "æœç´¢æ–‡æ¡£", "description": "æœç´¢å­¦ä¹ æ–‡æ¡£", "icon": "ğŸ”"},
            "get_document_stats": {"display_name": "æ–‡æ¡£ç»Ÿè®¡", "description": "è·å–æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯", "icon": "ğŸ“Š"},
            "get_recent_documents": {"display_name": "æœ€è¿‘æ–‡æ¡£", "description": "è·å–æœ€è¿‘é˜…è¯»çš„æ–‡æ¡£", "icon": "ğŸ“–"},
        }
        
        return tool_info_map.get(tool_name, {
            "display_name": tool_name,
            "description": "æ‰§è¡Œæ“ä½œ",
            "icon": "ğŸ”§"
        })
    
    def _parse_tool_output(self, output: Any) -> Dict[str, Any]:
        """
        è§£æå·¥å…·è¾“å‡ºï¼Œè½¬æ¢ä¸ºç»“æ„åŒ–æ•°æ®
        
        LangChain å·¥å…·å¯èƒ½è¿”å›å¤šç§æ ¼å¼ï¼š
        1. å­—ç¬¦ä¸²
        2. dict
        3. ToolMessage å¯¹è±¡ï¼ˆæœ‰ content å±æ€§ï¼‰
        4. ToolMessage çš„å­—ç¬¦ä¸²è¡¨ç¤º "content='...' name='...' tool_call_id='...'"
        """
        # å¦‚æœæ˜¯ LangChain çš„æ¶ˆæ¯å¯¹è±¡ï¼Œæå– content
        if hasattr(output, 'content'):
            content = output.content
            return {"success": True, "message": content}
        
        if isinstance(output, str):
            # æ£€æŸ¥æ˜¯å¦æ˜¯ ToolMessage çš„å­—ç¬¦ä¸²è¡¨ç¤º
            # æ ¼å¼ç±»ä¼¼: content='...' name='...' tool_call_id='...'
            if output.startswith("content='") or "content='" in output:
                try:
                    # æå– content å­—æ®µçš„å€¼
                    import re
                    # åŒ¹é… content='...' æˆ– content="..."
                    match = re.search(r"content=['\"](.+?)['\"](?:\s+name=|\s*$)", output, re.DOTALL)
                    if match:
                        content = match.group(1)
                        # å¤„ç†è½¬ä¹‰å­—ç¬¦
                        content = content.replace('\\n', '\n').replace("\\'", "'").replace('\\"', '"')
                        return {"success": True, "message": content}
                except Exception:
                    pass
            
            # å°è¯•è§£æä¸º JSON
            try:
                return json.loads(output)
            except json.JSONDecodeError:
                return {"success": True, "message": output}
        
        elif isinstance(output, dict):
            # å¦‚æœ dict åŒ…å« content å­—æ®µï¼Œæå–å‡ºæ¥
            if 'content' in output:
                return {"success": True, "message": output['content']}
            return output
        
        else:
            return {"success": True, "data": str(output)}
    
    def _build_system_message(self, input_data: Dict[str, Any]) -> str:
        """æ„å»ºç³»ç»Ÿæ¶ˆæ¯å†…å®¹"""
        template = (
            LEARNING_COACH_PROMPT if self.mode == "coach"
            else READING_COMPANION_PROMPT
        )
        
        return template.format(
            user_profile=input_data.get("user_profile", "æ–°ç”¨æˆ·"),
            conversation_summary=input_data.get("conversation_summary", "æ–°å¯¹è¯"),
            current_time=input_data.get("current_time", ""),
            reading_context=input_data.get("reading_context", "æ— "),
        )
    
    def _prepare_input(
        self,
        message: str,
        context: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """å‡†å¤‡ Agent è¾“å…¥"""
        from datetime import datetime
        
        # è·å–ç”¨æˆ·ç”»åƒ
        user_profile = self.memory.get_user_profile_summary()
        
        # è·å–å¯¹è¯æ‘˜è¦
        conversation_summary = self.memory.get_conversation_summary()
        
        # è·å–èŠå¤©å†å²
        chat_history = self.memory.get_chat_history(limit=10)
        
        # æ„å»ºè¾“å…¥
        input_data = {
            "input": message,
            "chat_history": chat_history,
            "user_profile": user_profile,
            "conversation_summary": conversation_summary,
            "current_time": datetime.now().strftime("%Y-%m-%d %H:%M"),
        }
        
        # æ·»åŠ æ¨¡å¼ç‰¹å®šçš„ä¸Šä¸‹æ–‡
        if self.mode == "reader" and context:
            input_data["reading_context"] = json.dumps(
                context, ensure_ascii=False, indent=2
            )
        else:
            input_data["reading_context"] = "æ— "
        
        return input_data
    
    async def _analyze_and_evolve(
        self,
        user_message: str,
        result: Dict[str, Any],
    ):
        """
        åˆ†æå¯¹è¯å¹¶æ›´æ–°ç”¨æˆ·ç”»åƒï¼ˆè¿›åŒ–æœºåˆ¶ï¼‰
        
        è¿™æ˜¯ Agent è‡ªæˆ‘è¿›åŒ–çš„æ ¸å¿ƒï¼š
        1. åˆ†æç”¨æˆ·çš„å­¦ä¹ åå¥½
        2. è¯†åˆ«ç”¨æˆ·çš„çŸ¥è¯†æ°´å¹³
        3. è®°å½•ç”¨æˆ·çš„å…´è¶£é¢†åŸŸ
        4. ä¼˜åŒ–äº¤äº’ç­–ç•¥
        """
        try:
            # æå–å…³é”®ä¿¡æ¯
            insights = await self._extract_insights(user_message, result)
            
            if insights:
                # æ›´æ–°ç”¨æˆ·ç”»åƒ
                await self.memory.update_user_profile(insights)
                
        except Exception as e:
            # è¿›åŒ–å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
            print(f"è¿›åŒ–åˆ†æå¤±è´¥: {e}")
    
    async def _extract_insights(
        self,
        user_message: str,
        result: Dict[str, Any],
    ) -> Optional[Dict[str, Any]]:
        """ä»å¯¹è¯ä¸­æå–ç”¨æˆ·æ´å¯Ÿ"""
        # ä½¿ç”¨å½“å‰ LLM åˆ†æå¯¹è¯ï¼ˆå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤ï¼‰
        llm = self._current_llm
        if not llm:
            # è·å–é»˜è®¤æ–‡æœ¬æ¨¡å‹
            llm = await self._get_llm_for_message(None)
        
        analysis_prompt = f"""åˆ†æä»¥ä¸‹å¯¹è¯ï¼Œæå–ç”¨æˆ·å­¦ä¹ ç›¸å…³çš„æ´å¯Ÿï¼š

ç”¨æˆ·æ¶ˆæ¯: {user_message}
åŠ©æ‰‹å›å¤: {result.get('output', '')[:500]}

è¯·ä»¥ JSON æ ¼å¼è¿”å›æ´å¯Ÿï¼ˆå¦‚æœæ²¡æœ‰æœ‰ä»·å€¼çš„æ´å¯Ÿè¿”å› nullï¼‰ï¼š
{{
    "learning_style": "ç”¨æˆ·çš„å­¦ä¹ é£æ ¼åå¥½ï¼ˆå¦‚æœ‰ï¼‰",
    "knowledge_level": "ç”¨æˆ·åœ¨æŸé¢†åŸŸçš„çŸ¥è¯†æ°´å¹³ï¼ˆå¦‚æœ‰ï¼‰",
    "interests": ["ç”¨æˆ·æ„Ÿå…´è¶£çš„ä¸»é¢˜ï¼ˆå¦‚æœ‰ï¼‰"],
    "pain_points": ["ç”¨æˆ·é‡åˆ°çš„å›°éš¾ï¼ˆå¦‚æœ‰ï¼‰"],
    "preferences": "ç”¨æˆ·çš„äº¤äº’åå¥½ï¼ˆå¦‚æœ‰ï¼‰"
}}

åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚å¦‚æœæ²¡æœ‰æœ‰ä»·å€¼çš„æ´å¯Ÿï¼Œè¿”å› nullã€‚
"""
        
        try:
            response = await llm.ainvoke([HumanMessage(content=analysis_prompt)])
            content = response.content.strip()
            
            if content and content != "null":
                # å°è¯•è§£æ JSON
                if content.startswith("```"):
                    content = content.split("```")[1]
                    if content.startswith("json"):
                        content = content[4:]
                
                return json.loads(content)
        except Exception:
            pass
        
        return None
    
    async def get_suggestions(self) -> List[str]:
        """
        æ ¹æ®ç”¨æˆ·ç”»åƒç”Ÿæˆä¸ªæ€§åŒ–å»ºè®®
        
        è¿™æ˜¯è¿›åŒ–æœºåˆ¶çš„ä½“ç°ä¹‹ä¸€ï¼šæ ¹æ®ç§¯ç´¯çš„ç”¨æˆ·æ•°æ®æä¾›æ›´å¥½çš„å»ºè®®
        """
        profile = self.memory.get_user_profile()
        
        if not profile:
            return [
                "å¼€å§‹åˆ¶å®šä½ çš„å­¦ä¹ è®¡åˆ’å§ï¼",
                "å‘Šè¯‰æˆ‘ä½ æƒ³å­¦ä»€ä¹ˆ",
                "ä¸Šä¼ ä¸€å¼ é¢˜ç›®å›¾ç‰‡ï¼Œæˆ‘æ¥å¸®ä½ è§£ç­”",
            ]
        
        # ä½¿ç”¨å½“å‰ LLM æˆ–è·å–é»˜è®¤æ–‡æœ¬æ¨¡å‹
        llm = self._current_llm
        if not llm:
            llm = await self._get_llm_for_message(None)
        
        # æ ¹æ®ç”¨æˆ·ç”»åƒç”Ÿæˆä¸ªæ€§åŒ–å»ºè®®
        suggestions_prompt = f"""æ ¹æ®ä»¥ä¸‹ç”¨æˆ·ç”»åƒï¼Œç”Ÿæˆ3æ¡ä¸ªæ€§åŒ–çš„å­¦ä¹ å»ºè®®ï¼š

ç”¨æˆ·ç”»åƒ:
{json.dumps(profile, ensure_ascii=False, indent=2)}

è¦æ±‚ï¼š
1. å»ºè®®è¦å…·ä½“å¯æ‰§è¡Œ
2. ä¸ç”¨æˆ·çš„å­¦ä¹ ç›®æ ‡ç›¸å…³
3. è€ƒè™‘ç”¨æˆ·çš„å­¦ä¹ é£æ ¼

ä»¥ JSON æ•°ç»„æ ¼å¼è¿”å›ï¼Œæ¯æ¡å»ºè®®ä¸è¶…è¿‡20ä¸ªå­—ï¼š
["å»ºè®®1", "å»ºè®®2", "å»ºè®®3"]
"""
        
        try:
            response = await llm.ainvoke([HumanMessage(content=suggestions_prompt)])
            return json.loads(response.content.strip())
        except Exception:
            return ["ç»§ç»­åŠ æ²¹å­¦ä¹ ï¼", "ä¿æŒå­¦ä¹ èŠ‚å¥", "æœ‰é—®é¢˜éšæ—¶é—®æˆ‘"]
