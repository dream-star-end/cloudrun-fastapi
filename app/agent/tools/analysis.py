"""
åˆ†æç›¸å…³å·¥å…·
"""

from typing import Optional, Type, TYPE_CHECKING
from pydantic import BaseModel, Field
from langchain_core.tools import BaseTool
from langchain_openai import ChatOpenAI

from ...config import settings

if TYPE_CHECKING:
    from ..memory import AgentMemory


class AnalyzeMistakeInput(BaseModel):
    """é”™é¢˜åˆ†æçš„è¾“å…¥å‚æ•°"""
    question: str = Field(description="é¢˜ç›®å†…å®¹")
    user_answer: str = Field(description="ç”¨æˆ·çš„ç­”æ¡ˆ")
    correct_answer: str = Field(default="", description="æ­£ç¡®ç­”æ¡ˆï¼ˆå¦‚æœçŸ¥é“ï¼‰")
    subject: str = Field(default="", description="å­¦ç§‘/é¢†åŸŸ")
    image_url: str = Field(default="", description="é¢˜ç›®å›¾ç‰‡URLï¼ˆå¦‚æœæœ‰ï¼‰")


class AnalyzeMistakeTool(BaseTool):
    """åˆ†æé”™é¢˜"""
    
    name: str = "analyze_mistake"
    description: str = """åˆ†æç”¨æˆ·çš„é”™é¢˜ï¼Œæ‰¾å‡ºé”™è¯¯åŸå› å¹¶ç»™å‡ºæ”¹è¿›å»ºè®®ã€‚
    å½“ç”¨æˆ·åšé”™é¢˜ç›®ã€ä¸ç†è§£ä¸ºä»€ä¹ˆé”™ã€æˆ–æƒ³è¦å¼„æ‡‚æŸé“é¢˜æ—¶ä½¿ç”¨ã€‚
    ä¼šåˆ†æé”™è¯¯ç±»å‹ã€çŸ¥è¯†æ¼æ´ï¼Œå¹¶æä¾›é’ˆå¯¹æ€§çš„å­¦ä¹ å»ºè®®ã€‚"""
    args_schema: Type[BaseModel] = AnalyzeMistakeInput
    
    def _run(self, **kwargs) -> str:
        import asyncio
        return asyncio.run(self._arun(**kwargs))
    
    async def _arun(
        self,
        question: str,
        user_answer: str,
        correct_answer: str = "",
        subject: str = "",
        image_url: str = "",
    ) -> str:
        """å¼‚æ­¥åˆ†æé”™é¢˜"""
        
        llm = ChatOpenAI(
            model=settings.DEEPSEEK_MODEL,
            openai_api_key=settings.DEEPSEEK_API_KEY,
            openai_api_base=settings.DEEPSEEK_API_BASE,
            temperature=0.5,
        )
        
        prompt = f"""ä½œä¸ºå­¦ä¹ åˆ†æä¸“å®¶ï¼Œè¯·åˆ†æè¿™é“é”™é¢˜ï¼š

## é¢˜ç›®ä¿¡æ¯
- é¢˜ç›®: {question}
- å­¦ç§‘: {subject or 'æœªæŒ‡å®š'}
- ç”¨æˆ·ç­”æ¡ˆ: {user_answer}
- æ­£ç¡®ç­”æ¡ˆ: {correct_answer or 'æœªæä¾›'}

## åˆ†æè¦æ±‚
è¯·ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢è¿›è¡Œåˆ†æï¼š

1. **é”™è¯¯ç±»å‹**: åˆ¤æ–­æ˜¯è®¡ç®—é”™è¯¯ã€æ¦‚å¿µç†è§£é”™è¯¯ã€ç²—å¿ƒå¤§æ„è¿˜æ˜¯å…¶ä»–ç±»å‹
2. **é”™è¯¯åŸå› **: è¯¦ç»†åˆ†æä¸ºä»€ä¹ˆä¼šå‡ºé”™
3. **çŸ¥è¯†æ¼æ´**: æŒ‡å‡ºå¯èƒ½å­˜åœ¨çš„çŸ¥è¯†è–„å¼±ç‚¹
4. **æ­£ç¡®è§£æ³•**: ç»™å‡ºè¯¦ç»†çš„æ­£ç¡®è§£é¢˜æ­¥éª¤
5. **å­¦ä¹ å»ºè®®**: æä¾›å…·ä½“çš„æ”¹è¿›å»ºè®®å’Œç»ƒä¹ æ–¹å‘

è¯·ç”¨æ¸…æ™°çš„æ ¼å¼è¾“å‡ºåˆ†æç»“æœã€‚
"""
        
        response = await llm.ainvoke([{"role": "user", "content": prompt}])
        return f"ğŸ“Š é”™é¢˜åˆ†æï¼š\n\n{response.content}"


class AnalyzeLearningStatusInput(BaseModel):
    """å­¦æƒ…åˆ†æçš„è¾“å…¥å‚æ•°"""
    period: str = Field(
        default="week",
        description="åˆ†æå‘¨æœŸï¼šday(ä»Šæ—¥)/week(æœ¬å‘¨)/month(æœ¬æœˆ)/all(å…¨éƒ¨)"
    )


class AnalyzeLearningStatusTool(BaseTool):
    """åˆ†æå­¦ä¹ çŠ¶æ€"""
    
    name: str = "analyze_learning_status"
    description: str = """åˆ†æç”¨æˆ·çš„å­¦ä¹ çŠ¶æ€å’Œè¿›åº¦ã€‚
    å½“ç”¨æˆ·æƒ³äº†è§£è‡ªå·±çš„å­¦ä¹ æƒ…å†µã€éœ€è¦å­¦ä¹ å»ºè®®ã€æˆ–æƒ³çŸ¥é“è¿›æ­¥ç¨‹åº¦æ—¶ä½¿ç”¨ã€‚
    ä¼šåˆ†æå­¦ä¹ æ—¶é•¿ã€å®Œæˆä»»åŠ¡ã€çŸ¥è¯†æŒæ¡ç¨‹åº¦ç­‰ã€‚"""
    args_schema: Type[BaseModel] = AnalyzeLearningStatusInput
    
    user_id: str = ""
    memory: Optional["AgentMemory"] = None
    
    class Config:
        arbitrary_types_allowed = True
    
    def __init__(self, user_id: str, memory: "AgentMemory", **kwargs):
        super().__init__(**kwargs)
        self.user_id = user_id
        self.memory = memory
    
    def _run(self, period: str = "week") -> str:
        import asyncio
        return asyncio.run(self._arun(period))
    
    async def _arun(self, period: str = "week") -> str:
        """å¼‚æ­¥åˆ†æå­¦ä¹ çŠ¶æ€"""
        
        # è·å–ç”¨æˆ·ç”»åƒ
        profile = {}
        if self.memory:
            profile = self.memory.get_user_profile()
        
        llm = ChatOpenAI(
            model=settings.DEEPSEEK_MODEL,
            openai_api_key=settings.DEEPSEEK_API_KEY,
            openai_api_base=settings.DEEPSEEK_API_BASE,
            temperature=0.7,
        )
        
        prompt = f"""ä½œä¸ºå­¦ä¹ åˆ†æå¸ˆï¼Œè¯·æ ¹æ®ç”¨æˆ·ç”»åƒåˆ†æå­¦ä¹ çŠ¶æ€ï¼š

## ç”¨æˆ·ç”»åƒ
{profile}

## åˆ†æå‘¨æœŸ
{period}

## åˆ†æè¦æ±‚
è¯·æä¾›ä»¥ä¸‹åˆ†æï¼š

1. **å­¦ä¹ æ¦‚å†µ**: æ€»ç»“ç”¨æˆ·çš„æ•´ä½“å­¦ä¹ æƒ…å†µ
2. **è¿›æ­¥äº®ç‚¹**: æŒ‡å‡ºç”¨æˆ·åšå¾—å¥½çš„åœ°æ–¹
3. **å¾…æ”¹è¿›é¡¹**: éœ€è¦åŠ å¼ºçš„æ–¹é¢
4. **å­¦ä¹ å»ºè®®**: å…·ä½“çš„ä¸‹ä¸€æ­¥è¡ŒåŠ¨å»ºè®®
5. **æ¿€åŠ±è¯­**: ç»™ç”¨æˆ·ä¸€å¥é¼“åŠ±çš„è¯

è¯·ç”¨å‹å¥½ã€ç§¯æçš„è¯­æ°”è¾“å‡ºã€‚
"""
        
        response = await llm.ainvoke([{"role": "user", "content": prompt}])
        return f"ğŸ“ˆ å­¦ä¹ çŠ¶æ€åˆ†æï¼š\n\n{response.content}"

