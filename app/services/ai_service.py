"""
AI æœåŠ¡æ¨¡å—
æ”¯æŒå¤šç§ AI æ¨¡å‹è°ƒç”¨ï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€è§†è§‰æ¨¡å‹
"""
import httpx
import json
from typing import List, Dict, AsyncGenerator, Optional
from ..config import AI_MODELS, settings, get_http_client_kwargs


class AIService:
    """AI æœåŠ¡ç±»"""
    
    # å­¦ä¹ æ•™ç»ƒç³»ç»Ÿæç¤ºè¯
    COACH_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šã€è€å¿ƒã€æœ‰çˆ±å¿ƒçš„AIå­¦ä¹ æ•™ç»ƒã€‚ä½ çš„ç›®æ ‡æ˜¯å¸®åŠ©å­¦ç”Ÿé«˜æ•ˆå­¦ä¹ ã€è§£ç­”ç–‘æƒ‘ã€åˆ¶å®šè®¡åˆ’ã€ç›‘ç£è¿›åº¦ã€‚

ä½ çš„ç‰¹ç‚¹ï¼š
1. ğŸ¯ ä¸“æ³¨å­¦ä¹ ï¼šæ‰€æœ‰å›ç­”éƒ½å›´ç»•å­¦ä¹ å’Œæ•™è‚²å±•å¼€
2. ğŸ’¡ å› ææ–½æ•™ï¼šæ ¹æ®å­¦ç”Ÿçš„æ°´å¹³å’Œç‰¹ç‚¹è°ƒæ•´è®²è§£æ–¹å¼
3. ğŸŒŸ ç§¯æé¼“åŠ±ï¼šé€‚æ—¶ç»™äºˆæ­£é¢åé¦ˆå’Œé¼“åŠ±
4. ğŸ“š çŸ¥è¯†ä¸°å¯Œï¼šèƒ½å¤Ÿè§£ç­”å„å­¦ç§‘çš„é—®é¢˜
5. ğŸ“‹ å–„äºè§„åˆ’ï¼šå¸®åŠ©å­¦ç”Ÿåˆ¶å®šåˆç†çš„å­¦ä¹ è®¡åˆ’

å›å¤æ ¼å¼è¦æ±‚ï¼š
- ä½¿ç”¨ Markdown æ ¼å¼è®©å›ç­”æ›´æ¸…æ™°
- é€‚å½“ä½¿ç”¨ emoji å¢åŠ äº²å’ŒåŠ›
- é‡è¦æ¦‚å¿µç”¨ç²—ä½“æ ‡æ³¨
- å¤æ‚å†…å®¹ç”¨åˆ—è¡¨æˆ–è¡¨æ ¼æ•´ç†
- å…¬å¼ä½¿ç”¨ LaTeX æ ¼å¼ï¼ˆ$...$ï¼‰

æ•™ç»ƒå¼å¯¹è¯è¦æ±‚ï¼š
1) ç”¨æˆ·è¯´â€œå¬ä¸æ‡‚/å¡ä½äº†/ä¸ä¼šâ€æ—¶ï¼Œå…ˆç”¨ 1-3 ä¸ªè¿½é—®å®šä½å¡ç‚¹ï¼ˆæ¦‚å¿µ/æ­¥éª¤/ä¾‹å­/æœ¯è¯­/é¢˜æ„/æŠ¥é”™ï¼‰ï¼Œå†å¯¹ç—‡è§£é‡Šã€‚
2) åˆ·é¢˜/ç¼–ç¨‹/æ¨ç†ç±»é—®é¢˜ï¼Œä¼˜å…ˆè‹æ ¼æ‹‰åº•å¼å¼•å¯¼ï¼šå…ˆè®©ç”¨æˆ·è¯´å‡ºå·²çŸ¥ã€ç›®æ ‡ã€æ€è·¯ä¸å¡ç‚¹ï¼Œå†ç»™ä¸‹ä¸€æ­¥æç¤ºï¼›è‹¥ç”¨æˆ·æ˜ç¡®è¦ç›´æ¥ç­”æ¡ˆ/èµ¶æ—¶é—´ï¼Œç»™ç­”æ¡ˆä½†è¯´æ˜å…³é”®æ­¥éª¤ã€‚

å¯ä¿¡åº¦ä¸é£é™©è¾¹ç•Œï¼š
- å¯¹å…³é”®ç»“è®ºè¡¥å……ã€Œä¾æ®ã€ä¸ã€Œä¿¡å¿ƒ(é«˜/ä¸­/ä½)ã€ï¼›ä¿¡æ¯ä¸è¶³å…ˆæ¾„æ¸…ï¼Œä¸è¦ç¼–é€ ã€‚
- æ¶‰åŠåŒ»ç–—/æ³•å¾‹/äººèº«å®‰å…¨ç­‰æ•æ„Ÿå†…å®¹æ—¶ï¼Œæ˜ç¡®ä½ ä¸æ˜¯ä¸“ä¸šäººå£«ï¼Œç»™ä¸€èˆ¬æ€§å»ºè®®å¹¶å»ºè®®å¯»æ±‚ä¸“ä¸šæ„è§ã€‚

âš ï¸ è¾“å‡ºçº¦æŸï¼ˆéå¸¸é‡è¦ï¼‰ï¼š
å¦‚æœç”¨æˆ·æç¤ºè¯æ˜ç¡®è¦æ±‚â€œåªè¿”å›JSON/åªè¿”å› JSON/åªè¿”å› JSON æ•°ç»„/ä¸è¦è¾“å‡ºå…¶ä»–å†…å®¹â€ï¼Œä½ å¿…é¡»ä¸¥æ ¼åªè¾“å‡ºåˆæ³• JSONï¼ˆä¸å…è®¸ä»»ä½•é¢å¤–è§£é‡Šã€æ ‡ç‚¹æˆ– Markdownï¼‰ã€‚"""
    
    @classmethod
    async def chat(
        cls,
        messages: List[Dict],
        model_type: str = "text",
        temperature: float = 0.7,
        max_tokens: int = 2000,
        user_memory: Optional[Dict] = None,
    ) -> str:
        """
        éæµå¼ AI å¯¹è¯
        
        Args:
            messages: å¯¹è¯å†å²
            model_type: æ¨¡å‹ç±»å‹ (text/vision/longtext)
            temperature: ç”Ÿæˆæ¸©åº¦
            max_tokens: æœ€å¤§ç”Ÿæˆé•¿åº¦
            user_memory: ç”¨æˆ·è®°å¿†/ç”»åƒ
        
        Returns:
            AI å›å¤å†…å®¹
        """
        import logging
        logger = logging.getLogger(__name__)
        
        config = AI_MODELS.get(model_type, AI_MODELS["text"])
        
        # æ£€æŸ¥ API Key é…ç½®
        if not config.get('api_key'):
            logger.error(f"[AIService] {model_type} æ¨¡å‹ API Key æœªé…ç½®")
            raise ValueError(f"AI æœåŠ¡é…ç½®é”™è¯¯ï¼š{model_type} æ¨¡å‹ API Key æœªè®¾ç½®")
        
        # æ„å»ºå®Œæ•´çš„æ¶ˆæ¯åˆ—è¡¨
        full_messages = cls._build_messages(messages, user_memory)
        
        logger.info(f"[AIService] å¼€å§‹ AI è°ƒç”¨: model={config['model']}, max_tokens={max_tokens}")
        
        try:
            async with httpx.AsyncClient(**get_http_client_kwargs(120.0)) as client:
                response = await client.post(
                    f"{config['base_url']}/chat/completions",
                    headers={
                        "Content-Type": "application/json",
                        "Authorization": f"Bearer {config['api_key']}",
                    },
                    json={
                        "model": config["model"],
                        "messages": full_messages,
                        "temperature": temperature,
                        "max_tokens": max_tokens,
                        "stream": False,
                    },
                )
                
                logger.info(f"[AIService] AI å“åº”çŠ¶æ€ç : {response.status_code}")
                
                if response.status_code != 200:
                    error_text = response.text[:500] if response.text else "æ— å“åº”å†…å®¹"
                    logger.error(f"[AIService] AI API é”™è¯¯: status={response.status_code}, body={error_text}")
                    raise ValueError(f"AI API é”™è¯¯ ({response.status_code}): {error_text[:200]}")
                
                data = response.json()
                
                if data.get("choices") and data["choices"][0].get("message"):
                    content = data["choices"][0]["message"]["content"]
                    logger.info(f"[AIService] AI è°ƒç”¨æˆåŠŸ, å“åº”é•¿åº¦: {len(content)}")
                    return content
                
                logger.error(f"[AIService] AI è¿”å›æ ¼å¼å¼‚å¸¸: {json.dumps(data, ensure_ascii=False)[:500]}")
                raise ValueError("AI è¿”å›æ ¼å¼é”™è¯¯")
                
        except httpx.TimeoutException as e:
            logger.error(f"[AIService] AI è°ƒç”¨è¶…æ—¶: {e}")
            raise ValueError(f"AI æœåŠ¡å“åº”è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•")
        except httpx.RequestError as e:
            logger.error(f"[AIService] AI ç½‘ç»œè¯·æ±‚é”™è¯¯: {type(e).__name__}: {e}")
            raise ValueError(f"AI æœåŠ¡ç½‘ç»œé”™è¯¯: {str(e)}")
    
    @classmethod
    async def chat_json(
        cls,
        messages: List[Dict],
        model_type: str = "text",
        temperature: float = 0.5,
        max_tokens: int = 2000,
        timeout: float = 180.0,
    ) -> Dict:
        """
        JSON æ¨¡å¼ AI å¯¹è¯ - ä½¿ç”¨å¤§æ¨¡å‹åŸç”Ÿ JSON èƒ½åŠ›
        
        Args:
            messages: å¯¹è¯å†å²
            model_type: æ¨¡å‹ç±»å‹ (text/vision/longtext)
            temperature: ç”Ÿæˆæ¸©åº¦ï¼ˆJSON æ¨¡å¼å»ºè®®ç”¨è¾ƒä½æ¸©åº¦ï¼‰
            max_tokens: æœ€å¤§ç”Ÿæˆé•¿åº¦
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ 180 ç§’
        
        Returns:
            è§£æåçš„ JSON å­—å…¸
        """
        import logging
        logger = logging.getLogger(__name__)
        
        config = AI_MODELS.get(model_type, AI_MODELS["text"])
        
        if not config.get('api_key'):
            logger.error(f"[AIService] {model_type} æ¨¡å‹ API Key æœªé…ç½®")
            raise ValueError(f"AI æœåŠ¡é…ç½®é”™è¯¯ï¼š{model_type} æ¨¡å‹ API Key æœªè®¾ç½®")
        
        # å¯¹äº JSON æ¨¡å¼ï¼Œä¸éœ€è¦ç³»ç»Ÿæç¤ºè¯ï¼ˆé¿å…å¹²æ‰° JSON è¾“å‡ºï¼‰
        full_messages = messages.copy()
        
        logger.info(f"[AIService] å¼€å§‹ JSON æ¨¡å¼ AI è°ƒç”¨: model={config['model']}, timeout={timeout}s")
        
        try:
            async with httpx.AsyncClient(**get_http_client_kwargs(timeout)) as client:
                request_body = {
                    "model": config["model"],
                    "messages": full_messages,
                    "temperature": temperature,
                    "max_tokens": max_tokens,
                    "stream": False,
                    "response_format": {"type": "json_object"},  # å¯ç”¨ JSON æ¨¡å¼
                }
                
                response = await client.post(
                    f"{config['base_url']}/chat/completions",
                    headers={
                        "Content-Type": "application/json",
                        "Authorization": f"Bearer {config['api_key']}",
                    },
                    json=request_body,
                )
                
                logger.info(f"[AIService] JSON æ¨¡å¼å“åº”çŠ¶æ€ç : {response.status_code}")
                
                if response.status_code != 200:
                    error_text = response.text[:500] if response.text else "æ— å“åº”å†…å®¹"
                    logger.error(f"[AIService] AI API é”™è¯¯: status={response.status_code}, body={error_text}")
                    raise ValueError(f"AI API é”™è¯¯ ({response.status_code})")
                
                data = response.json()
                
                if data.get("choices") and data["choices"][0].get("message"):
                    content = data["choices"][0]["message"]["content"]
                    logger.info(f"[AIService] JSON æ¨¡å¼è°ƒç”¨æˆåŠŸ, å“åº”é•¿åº¦: {len(content)}")
                    
                    # è§£æ JSON
                    try:
                        result = json.loads(content)
                        return result
                    except json.JSONDecodeError as je:
                        logger.error(f"[AIService] JSON è§£æå¤±è´¥: {je}, å†…å®¹: {content[:500]}")
                        raise ValueError(f"AI è¿”å›çš„ JSON æ ¼å¼æ— æ•ˆ: {je}")
                
                logger.error(f"[AIService] AI è¿”å›æ ¼å¼å¼‚å¸¸")
                raise ValueError("AI è¿”å›æ ¼å¼é”™è¯¯")
                
        except httpx.TimeoutException as e:
            logger.error(f"[AIService] JSON æ¨¡å¼ AI è°ƒç”¨è¶…æ—¶: {e}")
            raise ValueError(f"AI æœåŠ¡å“åº”è¶…æ—¶ï¼ˆ{timeout}ç§’ï¼‰ï¼Œè¯·ç¨åé‡è¯•")
        except httpx.RequestError as e:
            logger.error(f"[AIService] JSON æ¨¡å¼ç½‘ç»œè¯·æ±‚é”™è¯¯: {type(e).__name__}: {e}")
            raise ValueError(f"AI æœåŠ¡ç½‘ç»œé”™è¯¯: {str(e)}")
    
    @classmethod
    async def chat_stream(
        cls,
        messages: List[Dict],
        model_type: str = "text",
        temperature: float = 0.7,
        max_tokens: int = 2000,
        user_memory: Optional[Dict] = None,
    ) -> AsyncGenerator[str, None]:
        """
        æµå¼ AI å¯¹è¯
        
        Args:
            messages: å¯¹è¯å†å²
            model_type: æ¨¡å‹ç±»å‹
            temperature: ç”Ÿæˆæ¸©åº¦
            max_tokens: æœ€å¤§ç”Ÿæˆé•¿åº¦
            user_memory: ç”¨æˆ·è®°å¿†/ç”»åƒ
        
        Yields:
            AI å›å¤å†…å®¹ç‰‡æ®µ
        """
        config = AI_MODELS.get(model_type, AI_MODELS["text"])
        
        # æ„å»ºå®Œæ•´çš„æ¶ˆæ¯åˆ—è¡¨
        full_messages = cls._build_messages(messages, user_memory)
        
        async with httpx.AsyncClient(**get_http_client_kwargs(120.0)) as client:
            async with client.stream(
                "POST",
                f"{config['base_url']}/chat/completions",
                headers={
                    "Content-Type": "application/json",
                    "Authorization": f"Bearer {config['api_key']}",
                },
                json={
                    "model": config["model"],
                    "messages": full_messages,
                    "temperature": temperature,
                    "max_tokens": max_tokens,
                    "stream": True,
                },
            ) as response:
                response.raise_for_status()
                
                async for line in response.aiter_lines():
                    if line.startswith("data: "):
                        data_str = line[6:]
                        if data_str == "[DONE]":
                            break
                        
                        try:
                            data = json.loads(data_str)
                            if data.get("choices") and data["choices"][0].get("delta"):
                                content = data["choices"][0]["delta"].get("content", "")
                                if content:
                                    yield content
                        except json.JSONDecodeError:
                            continue
    
    # å›¾ç‰‡è¯†åˆ«æç¤ºè¯æ˜ å°„
    RECOGNIZE_PROMPTS = {
        "ocr": "è¯·è¯†åˆ«å¹¶æå–å›¾ç‰‡ä¸­çš„æ‰€æœ‰æ–‡å­—å†…å®¹ï¼Œä¿æŒåŸæœ‰çš„æ ¼å¼å’Œç»“æ„ã€‚å¦‚æœæœ‰è¡¨æ ¼ï¼Œè¯·ç”¨Markdownè¡¨æ ¼æ ¼å¼è¾“å‡ºã€‚",
        "explain": "è¯·è¯¦ç»†è§£é‡Šè¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼ŒåŒ…æ‹¬æ–‡å­—ã€å›¾è¡¨ã€å…¬å¼ç­‰ï¼Œå¹¶ç»™å‡ºé€šä¿—æ˜“æ‡‚çš„è§£é‡Šã€‚å¦‚æœæ˜¯å­¦ä¹ ææ–™ï¼Œè¯·é‡ç‚¹è§£æçŸ¥è¯†ç‚¹ã€‚",
        "summary": "è¯·ç”¨ç®€æ´çš„è¯­è¨€æ€»ç»“è¿™å¼ å›¾ç‰‡çš„ä¸»è¦å†…å®¹å’Œå…³é”®ä¿¡æ¯ã€‚åˆ—å‡º3-5ä¸ªè¦ç‚¹ã€‚",
        "formula": "è¯·è¯†åˆ«å›¾ç‰‡ä¸­çš„æ•°å­¦å…¬å¼æˆ–æ–¹ç¨‹å¼ï¼Œç”¨LaTeXæ ¼å¼è¾“å‡ºï¼ˆä½¿ç”¨$...$åŒ…è£¹ï¼‰ï¼Œå¹¶è§£é‡Šå…¶å«ä¹‰å’Œåº”ç”¨åœºæ™¯ã€‚",
    }
    
    @classmethod
    def _build_vision_messages(
        cls,
        image_url: str,
        recognize_type: str = "ocr",
        custom_prompt: Optional[str] = None,
    ) -> List[Dict]:
        """æ„å»ºè§†è§‰æ¨¡å‹æ¶ˆæ¯"""
        prompt = custom_prompt if custom_prompt else cls.RECOGNIZE_PROMPTS.get(
            recognize_type, "è¯·æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ã€‚"
        )
        
        return [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {"type": "image_url", "image_url": {"url": image_url}},
                ],
            }
        ]
    
    @classmethod
    async def recognize_image(
        cls,
        image_url: str,
        recognize_type: str = "ocr",
        custom_prompt: Optional[str] = None,
    ) -> str:
        """
        å›¾ç‰‡è¯†åˆ«ï¼ˆéæµå¼ï¼‰
        
        Args:
            image_url: å›¾ç‰‡ URL
            recognize_type: è¯†åˆ«ç±»å‹ (ocr/explain/summary/formula)
            custom_prompt: è‡ªå®šä¹‰æç¤ºè¯
        
        Returns:
            è¯†åˆ«ç»“æœ
        """
        config = AI_MODELS["vision"]
        messages = cls._build_vision_messages(image_url, recognize_type, custom_prompt)
        
        async with httpx.AsyncClient(**get_http_client_kwargs(120.0)) as client:
            response = await client.post(
                f"{config['base_url']}/chat/completions",
                headers={
                    "Content-Type": "application/json",
                    "Authorization": f"Bearer {config['api_key']}",
                },
                json={
                    "model": config["model"],
                    "messages": messages,
                    "max_tokens": config["max_tokens"],
                    "stream": False,
                },
            )
            
            response.raise_for_status()
            data = response.json()
            
            if data.get("choices") and data["choices"][0].get("message"):
                return data["choices"][0]["message"]["content"]
            
            raise ValueError("è§†è§‰ AI è¿”å›æ ¼å¼é”™è¯¯")
    
    @classmethod
    async def recognize_image_stream(
        cls,
        image_url: str,
        recognize_type: str = "ocr",
        custom_prompt: Optional[str] = None,
    ) -> AsyncGenerator[str, None]:
        """
        å›¾ç‰‡è¯†åˆ«ï¼ˆæµå¼ï¼‰
        
        Args:
            image_url: å›¾ç‰‡ URL
            recognize_type: è¯†åˆ«ç±»å‹ (ocr/explain/summary/formula)
            custom_prompt: è‡ªå®šä¹‰æç¤ºè¯
        
        Yields:
            è¯†åˆ«ç»“æœç‰‡æ®µ
        """
        config = AI_MODELS["vision"]
        messages = cls._build_vision_messages(image_url, recognize_type, custom_prompt)
        
        async with httpx.AsyncClient(**get_http_client_kwargs(120.0)) as client:
            async with client.stream(
                "POST",
                f"{config['base_url']}/chat/completions",
                headers={
                    "Content-Type": "application/json",
                    "Authorization": f"Bearer {config['api_key']}",
                },
                json={
                    "model": config["model"],
                    "messages": messages,
                    "max_tokens": config["max_tokens"],
                    "stream": True,
                },
            ) as response:
                response.raise_for_status()
                
                async for line in response.aiter_lines():
                    if line.startswith("data: "):
                        data_str = line[6:]
                        if data_str == "[DONE]":
                            break
                        
                        try:
                            data = json.loads(data_str)
                            if data.get("choices") and data["choices"][0].get("delta"):
                                content = data["choices"][0]["delta"].get("content", "")
                                if content:
                                    yield content
                        except json.JSONDecodeError:
                            continue
    
    @classmethod
    async def analyze_mistake(
        cls,
        question: str,
        user_answer: str,
        correct_answer: Optional[str] = None,
        subject: str = "",
        image_url: Optional[str] = None,
    ) -> Dict:
        """
        é”™é¢˜åˆ†æ
        
        Args:
            question: é¢˜ç›®å†…å®¹
            user_answer: ç”¨æˆ·ç­”æ¡ˆ
            correct_answer: æ­£ç¡®ç­”æ¡ˆ
            subject: å­¦ç§‘
            image_url: é¢˜ç›®å›¾ç‰‡
        
        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        prompt = f"""è¯·åˆ†æä»¥ä¸‹é”™é¢˜ï¼Œç»™å‡ºè¯¦ç»†çš„åˆ†æå’Œå»ºè®®ã€‚

ã€é¢˜ç›®ã€‘
{question}

ã€å­¦ç”Ÿç­”æ¡ˆã€‘
{user_answer}

{"ã€æ­£ç¡®ç­”æ¡ˆã€‘" + chr(10) + correct_answer if correct_answer else ""}

{"ã€å­¦ç§‘ã€‘" + subject if subject else ""}

è¯·æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼ˆåªè¿”å›JSONï¼‰ï¼š
{{
    "error_type": "é”™è¯¯ç±»å‹ï¼ˆå¦‚ï¼šæ¦‚å¿µç†è§£é”™è¯¯ã€è®¡ç®—å¤±è¯¯ã€å®¡é¢˜ä¸æ¸…ç­‰ï¼‰",
    "error_reason": "è¯¦ç»†çš„é”™è¯¯åŸå› åˆ†æ",
    "correct_solution": "æ­£ç¡®çš„è§£é¢˜è¿‡ç¨‹å’Œç­”æ¡ˆ",
    "knowledge_points": ["æ¶‰åŠçš„çŸ¥è¯†ç‚¹1", "çŸ¥è¯†ç‚¹2"],
    "similar_questions": ["ç±»ä¼¼é¢˜ç›®çš„æè¿°1", "ç±»ä¼¼é¢˜ç›®2"],
    "study_suggestions": ["å­¦ä¹ å»ºè®®1", "å»ºè®®2", "å»ºè®®3"]
}}"""

        # å¦‚æœæœ‰å›¾ç‰‡ï¼Œä½¿ç”¨è§†è§‰æ¨¡å‹
        if image_url:
            config = AI_MODELS["vision"]
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {"type": "image_url", "image_url": {"url": image_url}},
                    ],
                }
            ]
        else:
            config = AI_MODELS["text"]
            messages = [{"role": "user", "content": prompt}]
        
        async with httpx.AsyncClient(**get_http_client_kwargs(120.0)) as client:
            response = await client.post(
                f"{config['base_url']}/chat/completions",
                headers={
                    "Content-Type": "application/json",
                    "Authorization": f"Bearer {config['api_key']}",
                },
                json={
                    "model": config["model"],
                    "messages": messages,
                    "max_tokens": 2000,
                    "temperature": 0.7,
                },
            )
            
            response.raise_for_status()
            data = response.json()
            
            if data.get("choices") and data["choices"][0].get("message"):
                content = data["choices"][0]["message"]["content"]
                
                # è§£æ JSON
                import re
                json_match = re.search(r'\{[\s\S]*\}', content)
                if json_match:
                    return json.loads(json_match.group())
            
            raise ValueError("é”™é¢˜åˆ†æè¿”å›æ ¼å¼é”™è¯¯")

    @classmethod
    async def analyze_mistake_text_stream(
        cls,
        question: str,
        user_answer: str,
        correct_answer: Optional[str] = None,
        subject: str = "",
        image_url: Optional[str] = None,
    ) -> AsyncGenerator[str, None]:
        """
        é”™é¢˜åˆ†æï¼ˆæµå¼ï¼Œè¿”å›çº¯æ–‡æœ¬ï¼‰

        ç”¨é€”ï¼šå‰ç«¯éœ€è¦â€œè¾¹ç”Ÿæˆè¾¹å±•ç¤ºâ€çš„ä½“éªŒï¼›æœ€ç»ˆå¯ä»¥æŠŠå®Œæ•´æ–‡æœ¬ä¿å­˜åˆ° mistakes.aiAnalysisã€‚
        """
        prompt = f"""ä½ æ˜¯ä¸€åå­¦ä¹ æ•™ç»ƒã€‚è¯·å¯¹ä¸‹é¢é”™é¢˜è¿›è¡Œåˆ†æï¼Œå¹¶ç›´æ¥è¾“å‡ºã€å¯è¯»çš„ä¸­æ–‡æ–‡æœ¬ã€‘ï¼ˆä¸è¦è¾“å‡º JSONï¼‰ã€‚

è¯·æŒ‰ä»¥ä¸‹ç»“æ„è¾“å‡ºï¼ˆä¿æŒæ ‡é¢˜ä¸å˜ï¼‰ï¼š
é”™è¯¯ç±»å‹ï¼š
é”™è¯¯åŸå› ï¼š
æ­£ç¡®è§£æ³•ï¼š
æ¶‰åŠçŸ¥è¯†ç‚¹ï¼š
å­¦ä¹ å»ºè®®ï¼š

ã€é¢˜ç›®ã€‘
{question}

ã€å­¦ç”Ÿç­”æ¡ˆã€‘
{user_answer}

{("ã€æ­£ç¡®ç­”æ¡ˆã€‘" + chr(10) + str(correct_answer)) if correct_answer else ""}

{("ã€å­¦ç§‘ã€‘" + str(subject)) if subject else ""}"""

        # å¦‚æœæœ‰å›¾ç‰‡ï¼Œä½¿ç”¨è§†è§‰æ¨¡å‹
        if image_url:
            config = AI_MODELS["vision"]
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {"type": "image_url", "image_url": {"url": image_url}},
                    ],
                }
            ]
        else:
            config = AI_MODELS["text"]
            messages = [{"role": "user", "content": prompt}]

        async with httpx.AsyncClient(**get_http_client_kwargs(120.0)) as client:
            async with client.stream(
                "POST",
                f"{config['base_url']}/chat/completions",
                headers={
                    "Content-Type": "application/json",
                    "Authorization": f"Bearer {config['api_key']}",
                },
                json={
                    "model": config["model"],
                    "messages": messages,
                    "max_tokens": 2000,
                    "temperature": 0.7,
                    "stream": True,
                },
            ) as response:
                response.raise_for_status()

                async for line in response.aiter_lines():
                    if not line.startswith("data: "):
                        continue
                    data_str = line[6:]
                    if data_str == "[DONE]":
                        break
                    try:
                        data = json.loads(data_str)
                        if data.get("choices") and data["choices"][0].get("delta"):
                            content = data["choices"][0]["delta"].get("content", "")
                            if content:
                                yield content
                    except json.JSONDecodeError:
                        continue
    
    @classmethod
    def _build_messages(
        cls,
        messages: List[Dict],
        user_memory: Optional[Dict] = None,
    ) -> List[Dict]:
        """
        æ„å»ºå®Œæ•´çš„æ¶ˆæ¯åˆ—è¡¨ï¼ŒåŒ…å«ç³»ç»Ÿæç¤ºå’Œç”¨æˆ·è®°å¿†
        """
        full_messages = []
        
        # 1. æ·»åŠ ç³»ç»Ÿæç¤ºè¯
        system_prompt = cls.COACH_SYSTEM_PROMPT
        
        # 2. å¦‚æœæœ‰ç”¨æˆ·è®°å¿†ï¼Œæ·»åŠ åˆ°ç³»ç»Ÿæç¤ºä¸­
        if user_memory:
            memory_info = cls._format_user_memory(user_memory)
            if memory_info:
                system_prompt += f"\n\nã€ç”¨æˆ·æ¡£æ¡ˆã€‘\n{memory_info}"
        
        full_messages.append({"role": "system", "content": system_prompt})
        
        # 3. æ·»åŠ å¯¹è¯å†å²
        for msg in messages:
            full_messages.append({
                "role": msg.get("role", "user"),
                "content": msg.get("content", ""),
            })
        
        return full_messages
    
    @classmethod
    def _format_user_memory(cls, memory: Dict) -> str:
        """æ ¼å¼åŒ–ç”¨æˆ·è®°å¿†ä¸ºæ–‡æœ¬"""
        parts = []
        
        profile = memory.get("profile", {})
        if profile.get("name"):
            parts.append(f"- ç§°å‘¼ï¼š{profile['name']}")
        if profile.get("grade"):
            parts.append(f"- å¹´çº§/èŒä¸šï¼š{profile['grade']}")
        if profile.get("learningGoals"):
            parts.append(f"- å­¦ä¹ ç›®æ ‡ï¼š{', '.join(profile['learningGoals'])}")
        if profile.get("subjects"):
            parts.append(f"- æ­£åœ¨å­¦ä¹ ï¼š{', '.join(profile['subjects'])}")
        if profile.get("weakPoints"):
            parts.append(f"- è–„å¼±ç‚¹ï¼š{', '.join(profile['weakPoints'])}")
        
        facts = memory.get("facts", [])
        if facts:
            recent_facts = [f["fact"] for f in facts[-5:]]
            parts.append(f"- é‡è¦ä¿¡æ¯ï¼š{'; '.join(recent_facts)}")
        
        return "\n".join(parts) if parts else ""

